{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition on French CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qhedRYXtKleJ",
    "outputId": "0595d6ac-ccc7-4316-f427-563c5328f28f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1rJyyZjEkJj"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "from torch import autograd\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import time\n",
    "import _pickle as cPickle\n",
    "\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RxTeDu6YK7IK"
   },
   "outputs": [],
   "source": [
    "os.chdir('drive/MyDrive/Code NER CV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0dn60p0EkJk"
   },
   "source": [
    "##### Define constants and paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4WK5-Rk9EkJl"
   },
   "outputs": [],
   "source": [
    "#parameters for the Model\n",
    "parameters = OrderedDict()\n",
    "parameters['train'] = \"data/train.conllu\" #Path to train file\n",
    "parameters['test'] = \"data/test.conllu\" #Path to test file\n",
    "parameters['tag_scheme'] = \"BIO\"\n",
    "parameters['lower'] = True # Boolean variable to control lowercasing of words\n",
    "parameters['zeros'] =  True # Boolean variable to control replacement of  all digits by 0 \n",
    "parameters['char_dim'] = 30 #Char embedding dimension\n",
    "parameters['word_dim'] = 300 #Word embedding dimension\n",
    "parameters['word_lstm_dim'] = 200 #Token LSTM hidden layer size\n",
    "parameters['embedding_path'] = \"wiki.multi.fr.vec.txt\" #Path to pretrained embeddings\n",
    "parameters['dropout'] = 0.5 \n",
    "parameters['epoch'] =  50 #Number of epochs to run\"\n",
    "parameters['weights'] = \"\" #path to Pretrained for from a previous run\n",
    "parameters['name'] = \"self-trained-model-CV\" # Model name\n",
    "parameters['gradient_clip']=5.0\n",
    "models_path = \"./models/\" #path to saved models\n",
    "\n",
    "#GPU\n",
    "parameters['use_gpu'] = torch.cuda.is_available() #GPU Check\n",
    "use_gpu = parameters['use_gpu']\n",
    "\n",
    "parameters['reload'] = \"./models/self-trained-model-CV\" \n",
    "\n",
    "#Constants\n",
    "START_TAG = '<START>'\n",
    "STOP_TAG = '<STOP>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tptgHaN7EkJm"
   },
   "outputs": [],
   "source": [
    "#To stored model\n",
    "name = parameters['name']\n",
    "model_name = models_path + name #get_name(parameters)\n",
    "\n",
    "if not os.path.exists(models_path):\n",
    "    os.makedirs(models_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Wikineural dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R9o822tgFoiM",
    "outputId": "e2e733ab-8d73-43ab-d508-f8e5d581e5f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting conllu\n",
      "  Downloading conllu-4.4.1-py2.py3-none-any.whl (15 kB)\n",
      "Installing collected packages: conllu\n",
      "Successfully installed conllu-4.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHF0jGBfEkJo"
   },
   "outputs": [],
   "source": [
    "from conllu import parse as conllu_parse\n",
    "\n",
    "def store_sentences(path_data):\n",
    "    sentences = []\n",
    "    with open(path_data, encoding='utf-8') as reader:\n",
    "        lines = conllu_parse(reader.read())\n",
    "    for line in lines :\n",
    "        sentence = []\n",
    "        for i in range(len(line)) :\n",
    "            word = []            \n",
    "            word.append(line[i][\"form\"])\n",
    "            word.append(line[i][\"lemma\"])\n",
    "            sentence.append(word)\n",
    "        sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBL11inxEkJp"
   },
   "outputs": [],
   "source": [
    "train_sentences = store_sentences(parameters['train'])\n",
    "\n",
    "test_sentences = store_sentences(parameters['test'])\n",
    "val_sentences = list(reversed(test_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBGaAcFCEkJq"
   },
   "source": [
    "##### Create Mappings for Words, Characters and Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-vPpHPgEkJs"
   },
   "outputs": [],
   "source": [
    "def create_dico(item_list):\n",
    "    \"\"\"\n",
    "    Create a dictionary of items from a list of list of items.\n",
    "    \"\"\"\n",
    "    assert type(item_list) is list\n",
    "    dico = {}\n",
    "    for items in item_list:\n",
    "        for item in items:\n",
    "            if item not in dico:\n",
    "                dico[item] = 1\n",
    "            else:\n",
    "                dico[item] += 1\n",
    "    return dico\n",
    "\n",
    "def create_mapping(dico):\n",
    "    \"\"\"\n",
    "    Create a mapping (item to ID / ID to item) from a dictionary.\n",
    "    Items are ordered by decreasing frequency.\n",
    "    \"\"\"\n",
    "    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n",
    "    id_to_item = {i: v[0] for i, v in enumerate(sorted_items)}\n",
    "    item_to_id = {v: k for k, v in id_to_item.items()}\n",
    "    return item_to_id, id_to_item\n",
    "\n",
    "def word_mapping(sentences, lower):\n",
    "    \"\"\"\n",
    "    Create a dictionary and a mapping of words, sorted by frequency.\n",
    "    \"\"\"\n",
    "    words = [[x[0].lower() if lower else x[0] for x in s] for s in sentences]\n",
    "    dico = create_dico(words)\n",
    "    dico['<UNK>'] = 10000000 #UNK tag for unknown words\n",
    "    word_to_id, id_to_word = create_mapping(dico)\n",
    "    print(\"Found %i unique words (%i in total)\" % (\n",
    "        len(dico), sum(len(x) for x in words)\n",
    "    ))\n",
    "    return dico, word_to_id, id_to_word\n",
    "\n",
    "def char_mapping(sentences):\n",
    "    \"\"\"\n",
    "    Create a dictionary and mapping of characters, sorted by frequency.\n",
    "    \"\"\"\n",
    "    chars = [\"\".join([w[0] for w in s]) for s in sentences]\n",
    "    dico = create_dico(chars)\n",
    "    char_to_id, id_to_char = create_mapping(dico)\n",
    "    print(\"Found %i unique characters\" % len(dico))\n",
    "    return dico, char_to_id, id_to_char\n",
    "\n",
    "def tag_mapping(sentences):\n",
    "    \"\"\"\n",
    "    Create a dictionary and a mapping of tags, sorted by frequency.\n",
    "    \"\"\"\n",
    "    tags = [[word[-1] for word in s] for s in sentences]\n",
    "    dico = create_dico(tags)\n",
    "    dico[START_TAG] = -1\n",
    "    dico[STOP_TAG] = -2\n",
    "    tag_to_id, id_to_tag = create_mapping(dico)\n",
    "    print(\"Found %i unique named entity tags\" % len(dico))\n",
    "    return dico, tag_to_id, id_to_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SGBzXcQeEkJt",
    "outputId": "37c2725a-439e-4cd6-c723-e3c34bd735e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 101205 unique words (2601959 in total)\n",
      "Found 544 unique characters\n",
      "Found 11 unique named entity tags\n"
     ]
    }
   ],
   "source": [
    "dico_words, word_to_id, id_to_word = word_mapping(train_sentences, parameters['lower'])\n",
    "dico_chars, char_to_id, id_to_char = char_mapping(train_sentences)\n",
    "dico_tags, tag_to_id, id_to_tag = tag_mapping(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDI265KdEkJt",
    "outputId": "a8c9b6c5-5762-4e58-bbbd-750d238c8fba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<START>': -1,\n",
       " '<STOP>': -2,\n",
       " 'B-LOC': 80323,\n",
       " 'B-MISC': 22896,\n",
       " 'B-ORG': 18382,\n",
       " 'B-PER': 61780,\n",
       " 'I-LOC': 23731,\n",
       " 'I-MISC': 36388,\n",
       " 'I-ORG': 23090,\n",
       " 'I-PER': 60102,\n",
       " 'O': 2275267}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dico_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NvJe4BjhKi0x",
    "outputId": "5eacd175-360e-4b17-c5bb-6a8c0f1fcaf6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<START>': 9,\n",
       " '<STOP>': 10,\n",
       " 'B-LOC': 1,\n",
       " 'B-MISC': 7,\n",
       " 'B-ORG': 8,\n",
       " 'B-PER': 2,\n",
       " 'I-LOC': 5,\n",
       " 'I-MISC': 4,\n",
       " 'I-ORG': 6,\n",
       " 'I-PER': 3,\n",
       " 'O': 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9u1f-zdEkJu"
   },
   "source": [
    "##### Preparing final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uIuTu3ElEkJv"
   },
   "outputs": [],
   "source": [
    "def lower_case(x,lower=False):\n",
    "    if lower:\n",
    "        return x.lower()  \n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oabXzmHzKi0y",
    "outputId": "aee7a571-247d-4536-af16-b51c5e3cfef2"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(sentences, word_to_id, char_to_id, tag_to_id, lower=False):\n",
    "    \"\"\"\n",
    "    Prepare the dataset. Return a list of lists of dictionaries containing:\n",
    "        - word indexes\n",
    "        - word char indexes\n",
    "        - tag indexes\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for s in sentences:    \n",
    "        #Get the words of the sentence and store them in a list\n",
    "        str_words = [w[0] for w in s]\n",
    "        #Get the id of each word\n",
    "        words_id = [word_to_id[lower_case(w,lower) if lower_case(w,lower) in word_to_id else '<UNK>']\n",
    "                 for w in str_words]\n",
    "        #Get the character mapping of each word\n",
    "        chars = [[char_to_id[c] for c in w if c in char_to_id]\n",
    "                 for w in str_words]\n",
    "        #Get the tag associated to each word\n",
    "        tags = [tag_to_id[w[-1]] for w in s]\n",
    "        data.append({\n",
    "            'str_words': str_words,\n",
    "            'words_id': words_id,\n",
    "            'chars': chars,\n",
    "            'tags': tags,\n",
    "        })\n",
    "\n",
    "    return data\n",
    "\n",
    "train_data = prepare_dataset(\n",
    "    train_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
    ")\n",
    "\n",
    "val_data = prepare_dataset(\n",
    "    val_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
    ")\n",
    "\n",
    "test_data = prepare_dataset(\n",
    "    test_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6Si0YqhEkJw"
   },
   "source": [
    "##### Load Word Embeddings\n",
    "\n",
    "The Embeddings have been pretrained on a French wikipedia corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ozTcdz_OEkJx",
    "outputId": "6bd705b6-c301-4976-e5cd-be22a252a4e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 199041 pretrained embeddings.\n"
     ]
    }
   ],
   "source": [
    "#Store all the embeddings in a dict\n",
    "all_word_embeds = {}\n",
    "for i, line in enumerate(codecs.open(parameters['embedding_path'], 'r', 'utf-8')):\n",
    "    s = line.strip().split()\n",
    "    if len(s) == parameters['word_dim'] + 1:\n",
    "        all_word_embeds[s[0]] = np.array([float(i) for i in s[1:]])\n",
    "\n",
    "#Intializing Word Embedding Matrix. Allows to have a random embedding for words we don't have pre-trained embedding\n",
    "word_embeds = np.random.uniform(-np.sqrt(3/parameters['word_dim']), np.sqrt(3/parameters['word_dim']), (len(word_to_id), parameters['word_dim']))\n",
    "\n",
    "#Compute the embedding of every word in the training set\n",
    "for w in word_to_id:\n",
    "    if w in all_word_embeds:\n",
    "        word_embeds[word_to_id[w]] = all_word_embeds[w]\n",
    "    elif w.lower() in all_word_embeds:\n",
    "        word_embeds[word_to_id[w]] = all_word_embeds[w.lower()]\n",
    "\n",
    "print('Loaded %i pretrained embeddings.' % len(all_word_embeds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FW-C1XdPEkJz"
   },
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VdrNosIEkJ0"
   },
   "source": [
    "##### Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rroVOnLEkJ0"
   },
   "outputs": [],
   "source": [
    "def init_embedding(input_embedding):\n",
    "    \"\"\"\n",
    "    Initialize embedding\n",
    "    \"\"\"\n",
    "    bias = np.sqrt(3.0 / input_embedding.size(1))\n",
    "    nn.init.uniform_(input_embedding, -bias, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlihYhWdEkJ1"
   },
   "outputs": [],
   "source": [
    "def init_linear(input_linear):\n",
    "    \"\"\"\n",
    "    Initialize linear transformation\n",
    "    \"\"\"\n",
    "    bias = np.sqrt(6.0 / (input_linear.weight.size(0) + input_linear.weight.size(1)))\n",
    "    nn.init.uniform_(input_linear.weight, -bias, bias)\n",
    "    if input_linear.bias is not None:\n",
    "        input_linear.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Ryt8Q-TEkJ2"
   },
   "outputs": [],
   "source": [
    "def init_lstm(input_lstm):\n",
    "    \"\"\"\n",
    "    Initialize lstm          \n",
    "    \"\"\"\n",
    "    \n",
    "    # Weights init for forward layer\n",
    "    for ind in range(0, input_lstm.num_layers):\n",
    "        \n",
    "        ## Gets the weights Tensor from our model, for the input-hidden weights in our current layer\n",
    "        weight = eval('input_lstm.weight_ih_l' + str(ind))\n",
    "        \n",
    "        # Initialize the sampling range\n",
    "        sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "        \n",
    "        # Randomly sample from our samping range using uniform distribution and apply it to our current layer\n",
    "        nn.init.uniform_(weight, -sampling_range, sampling_range)\n",
    "        \n",
    "        # Similar to above but for the hidden-hidden weights of the current layer\n",
    "        weight = eval('input_lstm.weight_hh_l' + str(ind))\n",
    "        sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "        nn.init.uniform_(weight, -sampling_range, sampling_range)\n",
    "        \n",
    "        \n",
    "    # We do the above again, for the backward layer if we are using a bi-directional LSTM (our final model uses this)\n",
    "    if input_lstm.bidirectional:\n",
    "        for ind in range(0, input_lstm.num_layers):\n",
    "            weight = eval('input_lstm.weight_ih_l' + str(ind) + '_reverse')\n",
    "            sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "            nn.init.uniform_(weight, -sampling_range, sampling_range)\n",
    "            weight = eval('input_lstm.weight_hh_l' + str(ind) + '_reverse')\n",
    "            sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "            nn.init.uniform_(weight, -sampling_range, sampling_range)\n",
    "\n",
    "    # Bias initialization steps\n",
    "    \n",
    "    # We initialize them to zero except for the forget gate bias, which is initialized to 1\n",
    "    if input_lstm.bias:\n",
    "        for ind in range(0, input_lstm.num_layers):\n",
    "            bias = eval('input_lstm.bias_ih_l' + str(ind))\n",
    "            \n",
    "            # Initializing to zero\n",
    "            bias.data.zero_()\n",
    "            \n",
    "            # This is the range of indices for our forget gates for each LSTM cell\n",
    "            bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "            \n",
    "            #Similar for the hidden-hidden layer\n",
    "            bias = eval('input_lstm.bias_hh_l' + str(ind))\n",
    "            bias.data.zero_()\n",
    "            bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "            \n",
    "        # Similar to above, we do for backward layer if we are using a bi-directional LSTM \n",
    "        for ind in range(0, input_lstm.num_layers):\n",
    "            bias = eval('input_lstm.bias_ih_l' + str(ind) + '_reverse')\n",
    "            bias.data.zero_()\n",
    "            bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "            bias = eval('input_lstm.bias_hh_l' + str(ind) + '_reverse')\n",
    "            bias.data.zero_()\n",
    "            bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-HMAKRKKi05"
   },
   "source": [
    "## CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6W0XV0WKi05"
   },
   "source": [
    "Implement a CRF from scratch is difficult, we found help from https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-2vSRABEkJ4"
   },
   "source": [
    "##### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOYYPu0lEkJ4"
   },
   "outputs": [],
   "source": [
    "def log_sum_exp(vec):\n",
    "    '''\n",
    "    This function calculates the score explained above for the forward algorithm\n",
    "    vec 2D: 1 * tagset_size\n",
    "    '''\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "    \n",
    "def argmax(vec):\n",
    "    '''\n",
    "    This function returns the max index in a vector\n",
    "    '''\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return to_scalar(idx)\n",
    "\n",
    "def to_scalar(var):\n",
    "    '''\n",
    "    Function to convert pytorch tensor to a scalar\n",
    "    '''\n",
    "    return var.view(-1).data.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eR3u6mUMEkJ5"
   },
   "outputs": [],
   "source": [
    "def score_sentences(self, feats, tags):\n",
    "    # tags is ground_truth, a list of ints, length is len(sentence)\n",
    "    # feats is a 2D tensor, len(sentence) * tagset_size\n",
    "    r = torch.LongTensor(range(feats.size()[0]))\n",
    "    if self.use_gpu:\n",
    "        r = r.cuda()\n",
    "        pad_start_tags = torch.cat([torch.cuda.LongTensor([self.tag_to_ix[START_TAG]]), tags])\n",
    "        pad_stop_tags = torch.cat([tags, torch.cuda.LongTensor([self.tag_to_ix[STOP_TAG]])])\n",
    "    else:\n",
    "        pad_start_tags = torch.cat([torch.LongTensor([self.tag_to_ix[START_TAG]]), tags])\n",
    "        pad_stop_tags = torch.cat([tags, torch.LongTensor([self.tag_to_ix[STOP_TAG]])])\n",
    "\n",
    "    score = torch.sum(self.transitions[pad_stop_tags, pad_start_tags]) + torch.sum(feats[r, tags])\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sw_ehIXQEkJ6"
   },
   "source": [
    "##### Implementation of Forward Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bvur51NrEkJ6"
   },
   "outputs": [],
   "source": [
    "def forward_alg(self, feats):\n",
    "    '''\n",
    "    This function performs the forward algorithm explained above\n",
    "    '''\n",
    "    # calculate in log domain\n",
    "    # feats is len(sentence) * tagset_size\n",
    "    # initialize alpha with a Tensor with values all equal to -10000.\n",
    "    \n",
    "    # Do the forward algorithm to compute the partition function\n",
    "    init_alphas = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
    "    \n",
    "    # START_TAG has all of the score.\n",
    "    init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "    \n",
    "    # Wrap in a variable so that we will get automatic backprop\n",
    "    forward_var = autograd.Variable(init_alphas)\n",
    "    if self.use_gpu:\n",
    "        forward_var = forward_var.cuda()\n",
    "        \n",
    "    # Iterate through the sentence\n",
    "    for feat in feats:\n",
    "        # broadcast the emission score: it is the same regardless of\n",
    "        # the previous tag\n",
    "        emit_score = feat.view(-1, 1)\n",
    "        \n",
    "        # the ith entry of trans_score is the score of transitioning to\n",
    "        # next_tag from i\n",
    "        tag_var = forward_var + self.transitions + emit_score\n",
    "        \n",
    "        # The ith entry of next_tag_var is the value for the\n",
    "        # edge (i -> next_tag) before we do log-sum-exp\n",
    "        max_tag_var, _ = torch.max(tag_var, dim=1)\n",
    "        \n",
    "        # The forward variable for this tag is log-sum-exp of all the\n",
    "        # scores.\n",
    "        tag_var = tag_var - max_tag_var.view(-1, 1)\n",
    "        \n",
    "        # Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "        forward_var = max_tag_var + torch.log(torch.sum(torch.exp(tag_var), dim=1)).view(1, -1) # ).view(1, -1)\n",
    "    terminal_var = (forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]).view(1, -1)\n",
    "    alpha = log_sum_exp(terminal_var)\n",
    "    # Z(x)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orG8UJMaEkJ7"
   },
   "source": [
    "##### Implementation of Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqLQK32tEkJ8"
   },
   "outputs": [],
   "source": [
    "def viterbi_algo(self, feats):\n",
    "    '''\n",
    "    In this function, we implement the viterbi algorithm explained above.\n",
    "    A Dynamic programming based approach to find the best tag sequence\n",
    "    '''\n",
    "    backpointers = []\n",
    "    # analogous to forward\n",
    "    \n",
    "    # Initialize the viterbi variables in log space\n",
    "    init_vvars = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
    "    init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "    \n",
    "    # forward_var at step i holds the viterbi variables for step i-1\n",
    "    forward_var = Variable(init_vvars)\n",
    "    if self.use_gpu:\n",
    "        forward_var = forward_var.cuda()\n",
    "    for feat in feats:\n",
    "        next_tag_var = forward_var.view(1, -1).expand(self.tagset_size, self.tagset_size) + self.transitions\n",
    "        _, bptrs_t = torch.max(next_tag_var, dim=1)\n",
    "        bptrs_t = bptrs_t.squeeze().data.cpu().numpy() # holds the backpointers for this step\n",
    "        next_tag_var = next_tag_var.data.cpu().numpy() \n",
    "        viterbivars_t = next_tag_var[range(len(bptrs_t)), bptrs_t] # holds the viterbi variables for this step\n",
    "        viterbivars_t = Variable(torch.FloatTensor(viterbivars_t))\n",
    "        if self.use_gpu:\n",
    "            viterbivars_t = viterbivars_t.cuda()\n",
    "            \n",
    "        # Now add in the emission scores, and assign forward_var to the set\n",
    "        # of viterbi variables we just computed\n",
    "        forward_var = viterbivars_t + feat\n",
    "        backpointers.append(bptrs_t)\n",
    "\n",
    "    # Transition to STOP_TAG\n",
    "    terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "    terminal_var.data[self.tag_to_ix[STOP_TAG]] = -10000.\n",
    "    terminal_var.data[self.tag_to_ix[START_TAG]] = -10000.\n",
    "    best_tag_id = argmax(terminal_var.unsqueeze(0))\n",
    "    path_score = terminal_var[best_tag_id]\n",
    "    \n",
    "    # Follow the back pointers to decode the best path.\n",
    "    best_path = [best_tag_id]\n",
    "    for bptrs_t in reversed(backpointers):\n",
    "        best_tag_id = bptrs_t[best_tag_id]\n",
    "        best_path.append(best_tag_id)\n",
    "        \n",
    "    # Pop off the start tag (we dont want to return that to the caller)\n",
    "    start = best_path.pop()\n",
    "    assert start == self.tag_to_ix[START_TAG] # Sanity check\n",
    "    best_path.reverse()\n",
    "    return path_score, best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SIKrSiisEkJ8"
   },
   "outputs": [],
   "source": [
    "def forward_calc(self, sentence, chars, chars2_length, d):\n",
    "    \n",
    "    '''\n",
    "    The function calls viterbi decode and generates the \n",
    "    most probable sequence of tags for the sentence\n",
    "    '''\n",
    "    \n",
    "    # Get the emission scores from the BiLSTM\n",
    "    feats = self._get_lstm_features(sentence, chars, chars2_length, d)\n",
    "    # viterbi to get tag_seq\n",
    "    \n",
    "    # Find the best path, given the features.\n",
    "    score, tag_seq = self.viterbi_decode(feats)\n",
    "\n",
    "\n",
    "    return score, tag_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEuP6NDxEkJ9"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiKYMD6aEkJ-"
   },
   "outputs": [],
   "source": [
    "def get_lstm_features(self, sentence, chars2, chars2_length, d):    \n",
    "    \n",
    "    chars_embeds = self.char_embeds(chars2).unsqueeze(1)\n",
    "    #CNN\n",
    "    chars_cnn_out3 = self.char_cnn3(chars_embeds)\n",
    "    chars_embeds = nn.functional.max_pool2d(chars_cnn_out3,\n",
    "                                         kernel_size=(chars_cnn_out3.size(2), 1)).view(chars_cnn_out3.size(0), self.out_channels)\n",
    "    ## Loading word embeddings\n",
    "    embeds = self.word_embeds(sentence)\n",
    "    \n",
    "\n",
    "    ## We concatenate the word embeddings and the character level representation\n",
    "    embeds = torch.cat((embeds, chars_embeds), 1)\n",
    "\n",
    "    embeds = embeds.unsqueeze(1)\n",
    "\n",
    "    ## Dropout\n",
    "    embeds = self.dropout(embeds)\n",
    "\n",
    "    ## Word bilstm\n",
    "    lstm_out, _ = self.lstm(embeds)\n",
    "\n",
    "    ## Reshaping the outputs from the lstm layer\n",
    "    lstm_out = lstm_out.view(len(sentence), self.hidden_dim*2)\n",
    "\n",
    "    ## Dropout on the lstm output\n",
    "    lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "    ## Linear layer converts the ouput vectors to tag space\n",
    "    lstm_feats = self.hidden2tag(lstm_out)\n",
    "    \n",
    "    return lstm_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StWflkevEkJ_"
   },
   "outputs": [],
   "source": [
    "def get_neg_log_likelihood(self, sentence, tags, chars2, chars2_length, d):\n",
    "    # sentence, tags is a list of ints\n",
    "    # features is a 2D tensor, len(sentence) * self.tagset_size\n",
    "    feats = self._get_lstm_features(sentence, chars2, chars2_length, d)\n",
    "    forward_score = self._forward_alg(feats)\n",
    "    gold_score = self._score_sentence(feats, tags)\n",
    "    return forward_score - gold_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkq_-GBHEkJ_"
   },
   "source": [
    "##### Main Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlbZRSNQEkJ_"
   },
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim,\n",
    "                 char_to_ix=None, pre_word_embeds=None, char_out_dimension=25,char_embedding_dim=25, use_gpu=False\n",
    "                 ):\n",
    "        '''\n",
    "        Input parameters:                \n",
    "                vocab_size= Size of vocabulary (int)\n",
    "                tag_to_ix = Dictionary that maps NER tags to indices\n",
    "                embedding_dim = Dimension of word embeddings (int)\n",
    "                hidden_dim = The hidden dimension of the LSTM layer (int)\n",
    "                char_to_ix = Dictionary that maps characters to indices\n",
    "                pre_word_embeds = Numpy array which provides mapping from word embeddings to word indices\n",
    "                char_out_dimension = Output dimension from the CNN encoder for character\n",
    "                char_embedding_dim = Dimension of the character embeddings\n",
    "                use_gpu = defines availability of GPU \n",
    "        '''\n",
    "        \n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        \n",
    "        #parameter initialization for the model\n",
    "        self.use_gpu = use_gpu\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        self.out_channels = char_out_dimension\n",
    "\n",
    "        if char_embedding_dim is not None:\n",
    "            self.char_embedding_dim = char_embedding_dim\n",
    "            \n",
    "            #Initializing the character embedding layer\n",
    "            self.char_embeds = nn.Embedding(len(char_to_ix), char_embedding_dim)\n",
    "            init_embedding(self.char_embeds.weight)          \n",
    "\n",
    "                \n",
    "            #Performing CNN encoding on the character embeddings            \n",
    "            self.char_cnn3 = nn.Conv2d(in_channels=1, out_channels=self.out_channels, kernel_size=(3, char_embedding_dim), padding=(2,0))\n",
    "\n",
    "        #Creating Embedding layer with dimension of ( number of words * dimension of each word)\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if pre_word_embeds is not None:\n",
    "            #Initializes the word embeddings with pretrained word embeddings\n",
    "            self.pre_word_embeds = True\n",
    "            self.word_embeds.weight = nn.Parameter(torch.FloatTensor(pre_word_embeds))\n",
    "        else:\n",
    "            self.pre_word_embeds = False\n",
    "    \n",
    "        #Initializing the dropout layer, with dropout specificed in parameters\n",
    "        self.dropout = nn.Dropout(parameters['dropout'])       \n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim+self.out_channels, hidden_dim, bidirectional=True)\n",
    "        \n",
    "        #Initializing the lstm layer using predefined function for initialization\n",
    "        init_lstm(self.lstm)\n",
    "        \n",
    "        # Linear layer which maps the output of the bidirectional LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n",
    "        \n",
    "        #Initializing the linear layer using predefined function for initialization\n",
    "        init_linear(self.hidden2tag) \n",
    "\n",
    "        # Matrix of transition parameters.  Entry i,j is the score of transitioning *to* i *from* j.\n",
    "        # Matrix has a dimension of (total number of tags * total number of tags)\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.zeros(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer\n",
    "        # to the start tag and we never transfer from the stop tag\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "    #assigning the functions, which we have defined earlier\n",
    "    _score_sentence = score_sentences\n",
    "    _get_lstm_features = get_lstm_features\n",
    "    _forward_alg = forward_alg\n",
    "    viterbi_decode = viterbi_algo\n",
    "    neg_log_likelihood = get_neg_log_likelihood\n",
    "    forward = forward_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qsy9X7ExEkKA",
    "outputId": "5f55fd42-d069-4053-bc50-18ccd5a4abe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model initialized\n"
     ]
    }
   ],
   "source": [
    "#creating the model using the Class defined above\n",
    "model = BiLSTM_CRF(vocab_size=len(word_to_id),\n",
    "                   tag_to_ix=tag_to_id,\n",
    "                   embedding_dim=parameters['word_dim'],\n",
    "                   hidden_dim=parameters['word_lstm_dim'],\n",
    "                   use_gpu=use_gpu,\n",
    "                   char_to_ix=char_to_id,\n",
    "                   pre_word_embeds=word_embeds)\n",
    "print('model initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBrilWu81x8Z"
   },
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlHoG9IzEkKA"
   },
   "source": [
    "##### Training Paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8P4wFkEEkKA"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.015\n",
    "momentum = 0.9\n",
    "number_of_epochs = parameters['epoch'] \n",
    "gradient_clip = parameters['gradient_clip']\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "#variables which will used in training process\n",
    "losses = [] #list to store all losses\n",
    "best_val_F = -1.0 # Current best F-1 Score on Validation Set\n",
    "best_test_F = -1.0 # Current best F-1 Score on Test Set\n",
    "best_train_F = -1.0 # Current best F-1 Score on Train Set\n",
    "count = 0 #Counts the number of iterations\n",
    "trigger_times = 0\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OW0znqRBEkKB"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSzlWmpsEkKB"
   },
   "source": [
    "##### Helper functions for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2odHHcDEkKB"
   },
   "outputs": [],
   "source": [
    "def get_chunk_type(tok, idx_to_tag):\n",
    "    \"\"\"\n",
    "    The function takes in a chunk (\"B-PER\") and then splits it into the tag (PER) and its class (B)\n",
    "    as defined in BIOES\n",
    "    \n",
    "    Args:\n",
    "        tok: id of token, ex 4\n",
    "        idx_to_tag: dictionary {4: \"B-PER\", ...}\n",
    "\n",
    "    Returns:\n",
    "        tuple: \"B\", \"PER\"\n",
    "\n",
    "    \"\"\"\n",
    "    tag_name = idx_to_tag[tok]\n",
    "    tag_class = tag_name.split('-')[0]\n",
    "    tag_type = tag_name.split('-')[-1]\n",
    "    return tag_class, tag_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9d083OqBEkKB"
   },
   "outputs": [],
   "source": [
    "def get_chunks(seq, tags):\n",
    "    \"\"\"Given a sequence of tags, group entities and their position\n",
    "\n",
    "    Args:\n",
    "        seq: [4, 4, 0, 0, ...] sequence of labels\n",
    "        tags: dict[\"O\"] = 4\n",
    "\n",
    "    Returns:\n",
    "        list of (chunk_type, chunk_start, chunk_end)\n",
    "\n",
    "    Example:\n",
    "        seq = [4, 5, 0, 3]\n",
    "        tags = {\"B-PER\": 4, \"I-PER\": 5, \"B-LOC\": 3}\n",
    "        result = [(\"PER\", 0, 2), (\"LOC\", 3, 4)]\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # We assume by default the tags lie outside a named entity\n",
    "    default = tags[\"O\"]\n",
    "    \n",
    "    idx_to_tag = {idx: tag for tag, idx in tags.items()}\n",
    "    chunks = []\n",
    "    \n",
    "    chunk_type, chunk_start = None, None\n",
    "    for i, tok in enumerate(seq):\n",
    "        # End of a chunk 1\n",
    "        if tok == default and chunk_type is not None:\n",
    "            # Add a chunk.\n",
    "            chunk = (chunk_type, chunk_start, i)\n",
    "            chunks.append(chunk)\n",
    "            chunk_type, chunk_start = None, None\n",
    "\n",
    "        # End of a chunk + start of a chunk!\n",
    "        elif tok != default:\n",
    "            tok_chunk_class, tok_chunk_type = get_chunk_type(tok, idx_to_tag)\n",
    "            if chunk_type is None:\n",
    "                # Initialize chunk for each entity\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "            elif tok_chunk_type != chunk_type or tok_chunk_class == \"B\":\n",
    "                # If chunk class is B, i.e., its a beginning of a new named entity\n",
    "                # or, if the chunk type is different from the previous one, then we\n",
    "                # start labelling it as a new entity\n",
    "                chunk = (chunk_type, chunk_start, i)\n",
    "                chunks.append(chunk)\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # end condition\n",
    "    if chunk_type is not None:\n",
    "        chunk = (chunk_type, chunk_start, len(seq))\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtZHYelKEkKB"
   },
   "outputs": [],
   "source": [
    "def evaluating(model, datas, best_F, tag_to_id, dataset=\"Validation\"):\n",
    "    '''\n",
    "    The function takes as input the model, data and calcuates F-1 Score\n",
    "    It performs conditional updates \n",
    "     1) Flag to save the model \n",
    "     2) Best F-1 score\n",
    "    ,if the F-1 score calculated improves on the previous F-1 score\n",
    "    '''\n",
    "    # Initializations\n",
    "    prediction = [] # A list that stores predicted tags\n",
    "    save = False # Flag that tells us if the model needs to be saved\n",
    "    new_F = 0.0 # Variable to store the current F1-Score (may not be the best)\n",
    "    correct_preds, total_correct, total_preds = 0., 0., 0. # Count variables\n",
    "    \n",
    "    for data in datas:\n",
    "        ground_truth_id = data['tags']\n",
    "        words = data['str_words']\n",
    "        chars2 = data['chars']      \n",
    "\n",
    "        d = {}\n",
    "        # Padding the each word to max word size of that sentence\n",
    "        chars2_length = [len(c) for c in chars2]\n",
    "        char_maxl = max(chars2_length)\n",
    "        chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
    "        for i, c in enumerate(chars2):\n",
    "            chars2_mask[i, :chars2_length[i]] = c\n",
    "        chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "\n",
    "        dwords = Variable(torch.LongTensor(data['words_id']))\n",
    "        \n",
    "        # We are getting the predicted output from our model\n",
    "        if use_gpu:\n",
    "            val,out = model(dwords.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
    "        else:\n",
    "            val,out = model(dwords, chars2_mask, chars2_length, d)\n",
    "        predicted_id = out\n",
    "    \n",
    "        \n",
    "        # We use the get chunks function defined above to get the true chunks\n",
    "        # and the predicted chunks from true labels and predicted labels respectively\n",
    "        lab_chunks      = set(get_chunks(ground_truth_id,tag_to_id))\n",
    "        lab_pred_chunks = set(get_chunks(predicted_id,\n",
    "                                         tag_to_id))\n",
    "\n",
    "        # Updating the count variables\n",
    "        correct_preds += len(lab_chunks & lab_pred_chunks)\n",
    "        total_preds   += len(lab_pred_chunks)\n",
    "        total_correct += len(lab_chunks)\n",
    "    \n",
    "    # Calculating the F1-Score\n",
    "    p   = correct_preds / total_preds if correct_preds > 0 else 0\n",
    "    r   = correct_preds / total_correct if correct_preds > 0 else 0\n",
    "    new_F  = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
    "\n",
    "    \n",
    "    # If our current F1-Score is better than the previous best, we update the best\n",
    "    # to current F1 and we set the flag to indicate that we need to checkpoint this model\n",
    "    \n",
    "    if new_F>best_F:\n",
    "        best_F=new_F\n",
    "        save=True\n",
    "        \n",
    "    print(\"{}: new_F: {} best_F: {} \".format(dataset,new_F,best_F))\n",
    "\n",
    "\n",
    "    return best_F, new_F, save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkWjwShcEkKD"
   },
   "source": [
    "### Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R0RaxypmEkKD",
    "outputId": "698c2cec-4f93-4278-e495-32671b747282",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0 / 10000\n",
      "1000 / 10000\n",
      "2000 / 10000\n",
      "3000 / 10000\n",
      "4000 / 10000\n",
      "5000 / 10000\n",
      "6000 / 10000\n",
      "7000 / 10000\n",
      "8000 / 10000\n",
      "9000 / 10000\n",
      "Training: new_F: 0.8567582417582418 best_F: 1 \n",
      "Validation: new_F: 0.8180876172931898 best_F: 0.8180876172931898 \n",
      "Saving Model to  ./models/self-trained-model-CV\n",
      "epoch 2\n",
      "0 / 10000\n",
      "1000 / 10000\n",
      "2000 / 10000\n",
      "3000 / 10000\n",
      "4000 / 10000\n",
      "5000 / 10000\n",
      "6000 / 10000\n",
      "7000 / 10000\n",
      "8000 / 10000\n",
      "9000 / 10000\n",
      "Training: new_F: 0.9290221247325805 best_F: 1 \n",
      "Validation: new_F: 0.8541242937853107 best_F: 0.8541242937853107 \n",
      "Saving Model to  ./models/self-trained-model-CV\n",
      "epoch 3\n",
      "0 / 10000\n",
      "1000 / 10000\n",
      "2000 / 10000\n",
      "3000 / 10000\n",
      "4000 / 10000\n",
      "5000 / 10000\n",
      "6000 / 10000\n",
      "7000 / 10000\n",
      "8000 / 10000\n",
      "9000 / 10000\n",
      "Training: new_F: 0.9550951328037561 best_F: 1 \n",
      "Validation: new_F: 0.8635471483430521 best_F: 0.8635471483430521 \n",
      "Saving Model to  ./models/self-trained-model-CV\n",
      "epoch 4\n",
      "0 / 10000\n",
      "1000 / 10000\n",
      "2000 / 10000\n",
      "3000 / 10000\n",
      "4000 / 10000\n",
      "5000 / 10000\n",
      "6000 / 10000\n",
      "7000 / 10000\n",
      "8000 / 10000\n",
      "9000 / 10000\n",
      "Training: new_F: 0.9770653514180025 best_F: 1 \n",
      "Validation: new_F: 0.8879148764381903 best_F: 0.8879148764381903 \n",
      "Saving Model to  ./models/self-trained-model-CV\n",
      "epoch 5\n",
      "0 / 10000\n",
      "1000 / 10000\n",
      "2000 / 10000\n",
      "3000 / 10000\n",
      "4000 / 10000\n",
      "5000 / 10000\n",
      "6000 / 10000\n",
      "7000 / 10000\n",
      "8000 / 10000\n",
      "9000 / 10000\n",
      "Training: new_F: 0.9795795136411045 best_F: 1 \n",
      "Validation: new_F: 0.8800301737894218 best_F: 0.8879148764381903 \n",
      "trigger times: 1\n",
      "epoch 6\n",
      "0 / 10000\n",
      "1000 / 10000\n",
      "2000 / 10000\n",
      "3000 / 10000\n",
      "4000 / 10000\n",
      "5000 / 10000\n",
      "6000 / 10000\n",
      "7000 / 10000\n",
      "8000 / 10000\n",
      "9000 / 10000\n",
      "Training: new_F: 0.9881438076722981 best_F: 1 \n",
      "Validation: new_F: 0.8904141192755796 best_F: 0.8904141192755796 \n",
      "Saving Model to  ./models/self-trained-model-CV\n",
      "epoch 7\n",
      "0 / 10000\n",
      "1000 / 10000\n",
      "2000 / 10000\n",
      "3000 / 10000\n",
      "4000 / 10000\n",
      "5000 / 10000\n",
      "6000 / 10000\n",
      "7000 / 10000\n",
      "8000 / 10000\n",
      "9000 / 10000\n",
      "Training: new_F: 0.9890771715623204 best_F: 1 \n",
      "Validation: new_F: 0.8877542239178452 best_F: 0.8904141192755796 \n",
      "trigger times: 2\n",
      "epoch 8\n",
      "0 / 10000\n",
      "1000 / 10000\n",
      "2000 / 10000\n",
      "3000 / 10000\n",
      "4000 / 10000\n",
      "5000 / 10000\n",
      "6000 / 10000\n",
      "7000 / 10000\n",
      "8000 / 10000\n",
      "9000 / 10000\n",
      "Training: new_F: 0.9932709666830789 best_F: 1 \n",
      "Validation: new_F: 0.8865772581384687 best_F: 0.8904141192755796 \n",
      "trigger times: 3\n",
      "epoch 9\n",
      "0 / 10000\n",
      "1000 / 10000\n",
      "2000 / 10000\n",
      "3000 / 10000\n",
      "4000 / 10000\n",
      "5000 / 10000\n",
      "6000 / 10000\n",
      "7000 / 10000\n",
      "8000 / 10000\n",
      "9000 / 10000\n",
      "Training: new_F: 0.9949669019092948 best_F: 1 \n",
      "Validation: new_F: 0.89080640999914 best_F: 0.89080640999914 \n",
      "Saving Model to  ./models/self-trained-model-CV\n",
      "epoch 10\n",
      "0 / 10000\n",
      "1000 / 10000\n",
      "2000 / 10000\n",
      "3000 / 10000\n",
      "4000 / 10000\n",
      "5000 / 10000\n",
      "6000 / 10000\n",
      "7000 / 10000\n",
      "8000 / 10000\n",
      "9000 / 10000\n",
      "Training: new_F: 0.9954869943382292 best_F: 1 \n",
      "Validation: new_F: 0.887149752934792 best_F: 0.89080640999914 \n",
      "trigger times: 4\n",
      "epoch 11\n",
      "0 / 10000\n",
      "1000 / 10000\n",
      "2000 / 10000\n",
      "3000 / 10000\n",
      "4000 / 10000\n",
      "5000 / 10000\n",
      "6000 / 10000\n",
      "7000 / 10000\n",
      "8000 / 10000\n",
      "9000 / 10000\n",
      "Training: new_F: 0.9970457902511078 best_F: 1 \n",
      "Validation: new_F: 0.8909539379338037 best_F: 0.8909539379338037 \n",
      "Saving Model to  ./models/self-trained-model-CV\n",
      "epoch 12\n",
      "0 / 10000\n",
      "1000 / 10000\n",
      "2000 / 10000\n",
      "3000 / 10000\n",
      "4000 / 10000\n",
      "5000 / 10000\n",
      "6000 / 10000\n",
      "7000 / 10000\n",
      "8000 / 10000\n",
      "9000 / 10000\n",
      "Training: new_F: 0.9975111451467331 best_F: 1 \n",
      "Validation: new_F: 0.8934036334460428 best_F: 0.8934036334460428 \n",
      "Saving Model to  ./models/self-trained-model-CV\n",
      "epoch 13\n",
      "0 / 10000\n",
      "1000 / 10000\n",
      "2000 / 10000\n",
      "3000 / 10000\n",
      "4000 / 10000\n",
      "5000 / 10000\n",
      "6000 / 10000\n",
      "7000 / 10000\n",
      "8000 / 10000\n",
      "9000 / 10000\n",
      "Training: new_F: 0.997647445015866 best_F: 1 \n",
      "Validation: new_F: 0.8937464142283419 best_F: 0.8937464142283419 \n",
      "Saving Model to  ./models/self-trained-model-CV\n",
      "epoch 14\n",
      "0 / 10000\n",
      "1000 / 10000\n",
      "2000 / 10000\n",
      "3000 / 10000\n",
      "4000 / 10000\n",
      "5000 / 10000\n",
      "6000 / 10000\n",
      "7000 / 10000\n",
      "8000 / 10000\n",
      "9000 / 10000\n",
      "Training: new_F: 0.9981125365867003 best_F: 1 \n",
      "Validation: new_F: 0.8928458781362008 best_F: 0.8937464142283419 \n",
      "trigger times: 5\n",
      "epoch 15\n",
      "0 / 10000\n",
      "1000 / 10000\n",
      "2000 / 10000\n",
      "3000 / 10000\n",
      "4000 / 10000\n",
      "5000 / 10000\n",
      "6000 / 10000\n",
      "7000 / 10000\n",
      "8000 / 10000\n",
      "9000 / 10000\n",
      "Training: new_F: 0.9982218574750377 best_F: 1 \n",
      "Validation: new_F: 0.8939333065488146 best_F: 0.8939333065488146 \n",
      "Saving Model to  ./models/self-trained-model-CV\n",
      "epoch 16\n",
      "0 / 10000\n",
      "1000 / 10000\n",
      "2000 / 10000\n",
      "3000 / 10000\n",
      "4000 / 10000\n",
      "5000 / 10000\n",
      "6000 / 10000\n",
      "7000 / 10000\n",
      "8000 / 10000\n",
      "9000 / 10000\n",
      "Training: new_F: 0.998331372924474 best_F: 1 \n",
      "Validation: new_F: 0.8931464353421037 best_F: 0.8939333065488146 \n",
      "trigger times: 6\n",
      "epoch 17\n",
      "0 / 10000\n",
      "1000 / 10000\n",
      "2000 / 10000\n",
      "3000 / 10000\n",
      "4000 / 10000\n",
      "5000 / 10000\n",
      "6000 / 10000\n",
      "7000 / 10000\n",
      "8000 / 10000\n",
      "9000 / 10000\n",
      "Training: new_F: 0.9986870897155361 best_F: 1 \n",
      "Validation: new_F: 0.8928212923394615 best_F: 0.8939333065488146 \n",
      "trigger times: 7\n",
      "epoch 18\n",
      "0 / 10000\n",
      "1000 / 10000\n",
      "2000 / 10000\n",
      "3000 / 10000\n",
      "4000 / 10000\n",
      "5000 / 10000\n",
      "6000 / 10000\n",
      "7000 / 10000\n",
      "8000 / 10000\n",
      "9000 / 10000\n",
      "Training: new_F: 0.9987963672174197 best_F: 1 \n",
      "Validation: new_F: 0.8928017464238525 best_F: 0.8939333065488146 \n",
      "trigger times: 8\n",
      "epoch 19\n",
      "0 / 10000\n",
      "1000 / 10000\n",
      "2000 / 10000\n",
      "3000 / 10000\n",
      "4000 / 10000\n",
      "5000 / 10000\n",
      "6000 / 10000\n",
      "7000 / 10000\n",
      "8000 / 10000\n",
      "9000 / 10000\n",
      "Training: new_F: 0.998796498905908 best_F: 1 \n",
      "Validation: new_F: 0.8940859135125828 best_F: 0.8940859135125828 \n",
      "Saving Model to  ./models/self-trained-model-CV\n",
      "epoch 20\n",
      "0 / 10000\n",
      "1000 / 10000\n",
      "2000 / 10000\n",
      "3000 / 10000\n",
      "4000 / 10000\n",
      "5000 / 10000\n",
      "6000 / 10000\n",
      "7000 / 10000\n",
      "8000 / 10000\n",
      "9000 / 10000\n",
      "Training: new_F: 0.9989331728533523 best_F: 1 \n",
      "Validation: new_F: 0.8937367242666053 best_F: 0.8940859135125828 \n",
      "trigger times: 9\n",
      "epoch 21\n",
      "0 / 10000\n",
      "1000 / 10000\n",
      "2000 / 10000\n",
      "3000 / 10000\n",
      "4000 / 10000\n",
      "5000 / 10000\n",
      "6000 / 10000\n",
      "7000 / 10000\n",
      "8000 / 10000\n",
      "9000 / 10000\n",
      "Training: new_F: 0.9989331728533523 best_F: 1 \n",
      "Validation: new_F: 0.893557181805137 best_F: 0.8940859135125828 \n",
      "trigger times: 10\n",
      "epoch 22\n",
      "0 / 10000\n",
      "1000 / 10000\n",
      "2000 / 10000\n",
      "3000 / 10000\n",
      "4000 / 10000\n",
      "5000 / 10000\n",
      "6000 / 10000\n",
      "7000 / 10000\n",
      "8000 / 10000\n",
      "9000 / 10000\n",
      "Training: new_F: 0.9989331728533523 best_F: 1 \n",
      "Validation: new_F: 0.893781937187805 best_F: 0.8940859135125828 \n",
      "trigger times: 11\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "F_train = []\n",
    "F_val = []\n",
    "model.train(True)\n",
    "for epoch in range(1,number_of_epochs):\n",
    "    loss = 0.0\n",
    "    print('epoch',epoch)\n",
    "    for i, index in enumerate(np.random.permutation(len(train_data))):\n",
    "        if (i%1000==0):\n",
    "          print(i,'/',len(train_data))\n",
    "        count += 1\n",
    "        data = train_data[index]\n",
    "\n",
    "        ##gradient updates for each data entry\n",
    "        model.zero_grad()\n",
    "\n",
    "        sentence_in = data['words_id']\n",
    "        sentence_in = Variable(torch.LongTensor(sentence_in))\n",
    "        tags = data['tags']\n",
    "        chars2 = data['chars']\n",
    "        d = {}\n",
    "\n",
    "        ## Padding the each word to max word size of that sentence\n",
    "        chars2_length = [len(c) for c in chars2]\n",
    "        char_maxl = max(chars2_length)\n",
    "        chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
    "        for i, c in enumerate(chars2):\n",
    "            chars2_mask[i, :chars2_length[i]] = c\n",
    "        chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "\n",
    "\n",
    "        targets = torch.LongTensor(tags)\n",
    "        #we calculate the negative log-likelihood for the predicted tags using the predefined function\n",
    "        if use_gpu:\n",
    "            neg_log_likelihood = model.neg_log_likelihood(sentence_in.cuda(), targets.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
    "        else:\n",
    "            neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets, chars2_mask, chars2_length, d)\n",
    "        loss += neg_log_likelihood.item() / len(data['words_id'])\n",
    "\n",
    "        neg_log_likelihood.backward()\n",
    "\n",
    "        #we use gradient clipping to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "    #Evaluating on Validation dataset\n",
    "    model.train(False)\n",
    "\n",
    "    #Compute Fscore on training data\n",
    "    _, new_train_F, _ =  evaluating(model, train_data, 1 , tag_to_id, \"Training\")\n",
    "    best_val_F, new_val_F, save = evaluating(model, val_data, best_val_F, tag_to_id, \"Validation\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    #if the F-score on the validation set is greater than the best previous F-score, then we save the loadings\n",
    "    if save:\n",
    "        print(\"Saving Model to \", model_name)\n",
    "        torch.save(model.state_dict(), model_name)\n",
    "        trigger_times = 0\n",
    "    model.train(True)\n",
    "    F_train.append(new_train_F)\n",
    "    F_val.append(new_val_F)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    \n",
    "    if new_val_F < best_val_F:\n",
    "        trigger_times += 1\n",
    "        print('trigger times:', trigger_times)\n",
    "\n",
    "    if trigger_times >= patience:\n",
    "        print('Early stopping!')\n",
    "        break\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "nHvMRm-HRanU",
    "outputId": "dcca0888-072e-4882-ac36-3c4022905dd5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "plt.plot(F_train,label='Training')\n",
    "plt.plot(F_val,label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1-score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig.savefig('F1.png')\n",
    "\n",
    "\n",
    "plt.plot(losses,label='Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Negative log-likelihood')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig.savefig('loss.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KNBf6uGEkKE"
   },
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pxcNHpt7Ki1D",
    "outputId": "66418076-247b-4689-9b9b-5d7be84ee6dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model reloaded : ./models/self-trained-model-CV\n"
     ]
    }
   ],
   "source": [
    "#Reload a saved model, if parameter[\"reload\"] is set to a path\n",
    "if parameters['reload']:\n",
    "    model.load_state_dict(torch.load(parameters['reload'], map_location=torch.device('cpu')))\n",
    "    print(\"model reloaded :\", parameters['reload'])\n",
    "\n",
    "if use_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9PNRLDe0_vE",
    "outputId": "0edb709a-287b-43be-eeb3-ffe288a4ef8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: new_F: 0.9937891597581329 best_F: 1 \n",
      "Validation: new_F: 0.8762666973744818 best_F: 1 \n",
      "Test: new_F: 0.8767303512183959 best_F: 1 \n"
     ]
    }
   ],
   "source": [
    "_, new_train_F, _ =  evaluating(model, train_data, 1 , tag_to_id, \"Training\")\n",
    "_, new_val_F, _ =  evaluating(model, val_data, 1 , tag_to_id, \"Validation\")\n",
    "_, new_test_F, _ =  evaluating(model, test_data, 1 , tag_to_id, \"Test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVZCj7SqKi1D"
   },
   "source": [
    "# Fine-tuning on CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSjSuBU9Ki1D"
   },
   "source": [
    "Load CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5xTv0NCKi1E"
   },
   "outputs": [],
   "source": [
    "def tag_to_BIO(label):\n",
    "    '''\n",
    "    Transforms the tag scheme of a dictionnary of tag [name entity : tag]\n",
    "\n",
    "    Input parameters : \n",
    "        label : (str) last element of a tag [begin_tag, end_tag, label]\n",
    "    \n",
    "    Output :\n",
    "        new_label : (str) label adapted to the BIO tag scheme\n",
    "    '''\n",
    "\n",
    "    new_label = label\n",
    "    if label.startswith('U'):\n",
    "        label_tmp = list(label)\n",
    "        label_tmp[0] = 'B'\n",
    "        new_label = ''.join(label_tmp)\n",
    "    if label.startswith('L'):\n",
    "        label_tmp = list(label)\n",
    "        label_tmp[0] = 'I'\n",
    "        new_label = ''.join(label_tmp)\n",
    "       \n",
    "        \n",
    "    return new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6ejCssmKi1E"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_CV(path, text_key, label_key):\n",
    "    '''\n",
    "    Loads a jsonl file\n",
    "    \n",
    "    Input parameters : \n",
    "        path : (str) path of the jsonl file\n",
    "        text_key : (str) key corresponding to the text of a CV in jsonl file\n",
    "        label_key : (str) key corresponding to the labels of a CV in jsonl file\n",
    "    \n",
    "    Output :\n",
    "        train_sentences : (list) containing the text of a CV\n",
    "        tags : (list) containing the tags of a CV\n",
    "    '''\n",
    "    train_sentences =[]\n",
    "    tags = []\n",
    "    with open(path, 'r', encoding='utf8') as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "\n",
    "    for json_str in json_list:\n",
    "        #Each json_str corresponds to 1 CV\n",
    "        load = json.loads(json_str)\n",
    "        text = load[text_key]\n",
    "        tags.append(load[label_key])\n",
    "        train_sentences.append(text)\n",
    "    \n",
    "    #Apply the tag_to_BIO to each label \n",
    "    for tag in tags:\n",
    "        for t in tag :\n",
    "            label = t[-1]\n",
    "            #print(label)\n",
    "            t[-1] = tag_to_BIO(label)\n",
    "            if t[-1] == 'B-DateExp' or t[-1] == 'I-DateExp':        \n",
    "                t[-1] = 'O'\n",
    "    return train_sentences, tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJ3XscbOKi1G"
   },
   "outputs": [],
   "source": [
    "skills_data, tags_skills = load_CV('../NER annot/cv_remy_skills360.jsonl', 'text', 'labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ta5PdVqeKi1H"
   },
   "source": [
    "Create a dictionary storing a tag mapping for the tags of skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bQ4jOYLKKi1H",
    "outputId": "46bd578b-f67b-41bc-854c-b303574ce917"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-ANGLAIS': 1, 'B-NIVEAU_ANGLAIS': 2, 'B-OTHER_LANGUAGE': 3, 'B-SKILLS': 4, 'I-NIVEAU_ANGLAIS': 5, 'I-SKILLS': 6, '<START>': 7, '<STOP>': 8}\n"
     ]
    }
   ],
   "source": [
    "def create_dico_tags(tags):\n",
    "    #Store labels in a list\n",
    "    list_label = []\n",
    "    for tag in tags :\n",
    "        for t in tag:\n",
    "            label = t[-1]\n",
    "            if label not in list_label:\n",
    "                list_label.append(label)\n",
    "\n",
    "    #To be sure O has the value 0 because it was the case in the original training but don't know if useful or not\n",
    "    dico_label = {'O' : 10000000, '<START>': -1,\n",
    "     '<STOP>': -2}\n",
    "    for label in list_label:\n",
    "            if label not in dico_label:\n",
    "                dico_label[label] = 1\n",
    "            else:\n",
    "                dico_label[label] += 1\n",
    "    return dico_label\n",
    "            \n",
    "dico_label_skills = create_dico_tags(tags_skills)       \n",
    "tag_to_id_skills, id_to_tag_skills = create_mapping(dico_label_skills)\n",
    "\n",
    "print(tag_to_id_skills)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zVmYuT3Ki1I"
   },
   "outputs": [],
   "source": [
    "def train_val_test_split(data, tags, train_size=0.8,val_size=0.1,test_size=0.1):\n",
    "    train_size = int(len(data)*0.8)\n",
    "    val_size = int(len(data)*0.1)\n",
    "    test_size = int(len(data)*0.1)\n",
    "    \n",
    "    data_train, tags_train = data[:train_size], tags[:train_size]\n",
    "    data_val, tags_val = data[train_size:train_size+val_size], tags[train_size:train_size+val_size]\n",
    "    data_test, tags_test = data[-test_size:], tags[-test_size:]\n",
    "    return data_train, tags_train, data_val, tags_val, data_test, tags_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHA1h1-VKi1I"
   },
   "outputs": [],
   "source": [
    "train_size = int(len(skills_data)*0.8)\n",
    "val_size = int(len(skills_data)*0.1)\n",
    "test_size = int(len(skills_data)*0.1)\n",
    "\n",
    "skills_data_train, tags_skills_train, skills_data_val, tags_skills_val,skills_data_test, tags_skills_test = train_val_test_split(skills_data, tags_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fPovxwxkKi1I"
   },
   "outputs": [],
   "source": [
    "def extract_tag(sentences, tags) :\n",
    "    '''\n",
    "    Returns a dictionary containg the tags of a given CV\n",
    "    \n",
    "    Input parameters : \n",
    "        sentences_CV : str\n",
    "        tags_CV : list of tag_list. Each tag_list is a list [beginning char, ending char, label]\n",
    "        \n",
    "    Output :\n",
    "        dict_word_tag : dict whose keys are strings corresponding to the name entity and the value corresponds to the associated tag\n",
    "    '''\n",
    "    \n",
    "    dict_word_tag = {}\n",
    "    for i, tag in enumerate(tags):\n",
    "        start_tag = tag[0]\n",
    "        end_tag = tag[1]\n",
    "        tagged_word = sentences[start_tag : end_tag]\n",
    "        #Remove '\\n' from string\n",
    "        tagged_word = tagged_word.replace('\\n', ' ')\n",
    "        dict_word_tag[tagged_word] = tag[-1]\n",
    "        \n",
    "    return(dict_word_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Psz4MzITKi1J"
   },
   "outputs": [],
   "source": [
    "def adapt_format(sentences, tags, remove=True):\n",
    "    '''\n",
    "    In order to obtain the same format of data as the dataset we trained the model on, the sentences and associated tags need to be reshaped\n",
    "    \n",
    "    Input parameters : \n",
    "        train_sentences_CV : (list) containing the text of a CV\n",
    "        tags_CV : (list) containing the tags of a CV\n",
    "        \n",
    "    Ouput :\n",
    "        train_sentences : list of list of list. The macro list store the sentences. Each element of macro list (called meso list) corresponds to a CV. Each element of meso list corresponds to a list of 2 elements [words, label].\n",
    "    '''\n",
    "    #Init a condition\n",
    "    \n",
    "    special_char = ['.','(',')',':','-',',']\n",
    "    list_sentences = list()\n",
    "    for k, sentence in enumerate(sentences):\n",
    "        #Extract the tags of each CV\n",
    "        tagged_word = extract_tag(sentence, tags[k])\n",
    "        sub_list_sentence = list()\n",
    "        \n",
    "        for i,word in enumerate(sentence.split()):                     \n",
    "            add_char_after = False\n",
    "            begin_char, last_char = word[0], word[-1]\n",
    "                \n",
    "            if begin_char in special_char:\n",
    "                word = word[1:]\n",
    "                if not remove:\n",
    "                    word_and_label = list()\n",
    "                    #Add the beginning character to the list of tokens\n",
    "                    word_and_label.append(begin_char)\n",
    "                    word_and_label.append('O')\n",
    "                    sub_list_sentence.append(word_and_label)\n",
    "                \n",
    "            if last_char in special_char:\n",
    "                add_char_after = True\n",
    "                word = word[:-1]               \n",
    "                \n",
    "            word_and_label = list()\n",
    "            word_and_label.append(word)\n",
    "            if word in tagged_word:\n",
    "                word_and_label.append(tagged_word[word])\n",
    "            else :\n",
    "                word_and_label.append('O')\n",
    "            sub_list_sentence.append(word_and_label)\n",
    "            \n",
    "            if add_char_after and not remove:\n",
    "                word_and_label = list()\n",
    "                word_and_label.append(last_char)\n",
    "                word_and_label.append('O')\n",
    "                sub_list_sentence.append(word_and_label)\n",
    "                \n",
    "\n",
    "        #Remove Line Break\n",
    "        sent_no_line_break=[]\n",
    "        for sent in sub_list_sentence:\n",
    "            if sent[0]!='SAUTDELIGNE':\n",
    "                sent_no_line_break.append(sent)            \n",
    "        list_sentences.append(sent_no_line_break)\n",
    "    return list_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oCN4NneUKi1J"
   },
   "outputs": [],
   "source": [
    "train_sentences_skills_adapted = adapt_format(skills_data_train, tags_skills_train)\n",
    "val_sentences_skills_adapted = adapt_format(skills_data_val, tags_skills_val)\n",
    "test_sentences_skills_adapted = adapt_format(skills_data_test, tags_skills_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_oV83V5Ki1K"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(sentences, word_to_id, char_to_id, tag_to_id, lower=False):\n",
    "    \"\"\"\n",
    "    Prepare the dataset. Return a list of lists of dictionaries containing:\n",
    "        - word indexes\n",
    "        - word char indexes\n",
    "        - tag indexes\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for s in sentences:\n",
    "        #Get the words of the sentence and store them in a list (get rid of the tag associated to the word)\n",
    "       \n",
    "        str_words = [w[0] for w in s]\n",
    "        #Get the id of each word, looking for the lower case version of words. Manage unknown words\n",
    "        words_id = [word_to_id[lower_case(w,lower) if lower_case(w,lower) in word_to_id else '<UNK>']\n",
    "                 for w in str_words]\n",
    "        #Get the character mapping of each word\n",
    "        chars = [[char_to_id[c] for c in w if c in char_to_id]\n",
    "                 for w in str_words]\n",
    "        #Get the tag associated to each word\n",
    "        tags = [tag_to_id[w[-1]] for w in s]\n",
    "        data.append({\n",
    "            'str_words': str_words,\n",
    "            'words_id': words_id,\n",
    "            'chars': chars,\n",
    "            'tags': tags,\n",
    "        })\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H9Q689VzKi1L",
    "outputId": "34ab7116-b3d1-4902-b3d2-2ec876d48489"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288/ 36 /36 sentences in train / test.\n"
     ]
    }
   ],
   "source": [
    "train_data_skills = prepare_dataset(\n",
    "    train_sentences_skills_adapted, word_to_id, char_to_id, tag_to_id_skills, parameters['lower']\n",
    ")\n",
    "\n",
    "val_data_skills = prepare_dataset(\n",
    "    val_sentences_skills_adapted, word_to_id, char_to_id, tag_to_id_skills, parameters['lower']\n",
    ")\n",
    "\n",
    "test_data_skills = prepare_dataset(\n",
    "    test_sentences_skills_adapted, word_to_id, char_to_id, tag_to_id_skills, parameters['lower']\n",
    ")\n",
    "print(\"{}/ {} /{} sentences in train / test.\".format(len(train_data_skills),len(val_data_skills), len(test_data_skills)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gl68d9bzKi1L"
   },
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n8JO_fl4Ki1M",
    "outputId": "a2218e1b-6878-49c0-ecad-701f4da351a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized!!!\n"
     ]
    }
   ],
   "source": [
    "#creating the model using the Class defined above\n",
    "model = BiLSTM_CRF(vocab_size=len(word_to_id),\n",
    "                   tag_to_ix=tag_to_id,\n",
    "                   embedding_dim=parameters['word_dim'],\n",
    "                   hidden_dim=parameters['word_lstm_dim'],\n",
    "                   use_gpu=use_gpu,\n",
    "                   char_to_ix=char_to_id,\n",
    "                   pre_word_embeds=word_embeds)\n",
    "print(\"Model Initialized!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-UueweM5Ki1M",
    "outputId": "5b4648fb-453f-4c48-fac0-679cbec74262"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model reloaded : ./models/self-trained-model-CV\n"
     ]
    }
   ],
   "source": [
    "#Reload a saved model, if parameter[\"reload\"] is set to a path\n",
    "if parameters['reload']:\n",
    "    model.load_state_dict(torch.load(parameters['reload'], map_location=torch.device('cpu')))\n",
    "    print(\"model reloaded :\", parameters['reload'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctN1imW55Tz_"
   },
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzH09DHGKi1M"
   },
   "source": [
    "### Skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iUL-sjDKi1M"
   },
   "source": [
    "Add a FC layer to suit proper dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Opg6FUckKi1N"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "model_skills = copy.deepcopy(model)\n",
    "out_dim = len(tag_to_id_skills)\n",
    "model_skills.fc = nn.Linear(list(model.children())[-1].out_features, out_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1-nPf_pKi1N"
   },
   "source": [
    "Training of CV dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EluFh12lKi1N"
   },
   "outputs": [],
   "source": [
    "number_of_epochs = 100\n",
    "trigger_times = 0\n",
    "patience = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBzeqHFMKi1O"
   },
   "outputs": [],
   "source": [
    "def fine_tune(model, train_data, val_data, tag_to_id, model_name, number_of_epochs=number_of_epochs, trigger_times=trigger_times, patience=patience):\n",
    "    F_train = []\n",
    "    F_val = []\n",
    "    #Initialize\n",
    "    best_val_F = -1.0\n",
    "    count = 0\n",
    "    losses = []\n",
    "    learning_rate = 1e-3\n",
    "    momentum = 0.9\n",
    "    decay_rate = 0.05\n",
    "    gradient_clip = parameters['gradient_clip']\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    \n",
    "    \n",
    "    \n",
    "    tr = time.time()\n",
    "    model.train(True)\n",
    "    for epoch in range(1,number_of_epochs):\n",
    "        loss = 0.0\n",
    "        print('epoch',epoch)\n",
    "        for i, index in enumerate(np.random.permutation(len(train_data))):\n",
    "            count += 1\n",
    "            data = train_data[index]\n",
    "\n",
    "            ##gradient updates for each data entry\n",
    "            model.zero_grad()\n",
    "\n",
    "            sentence_in = data['words_id']\n",
    "            sentence_in = Variable(torch.LongTensor(sentence_in))\n",
    "            tags = data['tags']\n",
    "            chars2 = data['chars']\n",
    "\n",
    "            d = {}\n",
    "\n",
    "            ## Padding the each word to max word size of that sentence\n",
    "            chars2_length = [len(c) for c in chars2]\n",
    "            char_maxl = max(chars2_length)\n",
    "            chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
    "            for i, c in enumerate(chars2):\n",
    "                chars2_mask[i, :chars2_length[i]] = c\n",
    "            chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "\n",
    "\n",
    "            targets = torch.LongTensor(tags)\n",
    "            #we calculate the negative log-likelihood for the predicted tags using the predefined function\n",
    "            if use_gpu:\n",
    "                neg_log_likelihood = model.neg_log_likelihood(sentence_in.cuda(), targets.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
    "            else:\n",
    "                neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets, chars2_mask, chars2_length, d)\n",
    "            loss += neg_log_likelihood.item() / len(data['words_id'])\n",
    "\n",
    "            neg_log_likelihood.backward()\n",
    "\n",
    "            #we use gradient clipping to avoid exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        #Evaluating on Validation dataset\n",
    "        model.train(False)\n",
    "        _, new_train_F, _ =  evaluating(model, train_data, 1 , tag_to_id, \"Training\")\n",
    "        best_val_F, new_val_F, save = evaluating(model, val_data, best_val_F, tag_to_id, \"Validation\")\n",
    "\n",
    "        #if the F-score on the validation set is greater than the best previous F-score, then we save the loadings\n",
    "        if save:\n",
    "            print(\"Saving Model to \", model_name)\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "            trigger_times = 0\n",
    "        model.train(True)\n",
    "        losses.append(loss)\n",
    "        F_train.append(new_train_F)\n",
    "        F_val.append(new_val_F)\n",
    "\n",
    "        if new_val_F <= best_val_F:\n",
    "            trigger_times += 1\n",
    "            print('trigger times:', trigger_times)\n",
    "\n",
    "        if trigger_times >= patience:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "\n",
    "\n",
    "    return losses, F_train, F_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJOjjF5tKi1O"
   },
   "outputs": [],
   "source": [
    "losses_skills, F_train_skills , F_val_skills = fine_tune(model_skills, train_data_skills, val_data_skills, tag_to_id_skills, './models/self-trained-model-skills')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLn-LdYnz1f5"
   },
   "outputs": [],
   "source": [
    "plt.plot(F_train_skills,label='Training')\n",
    "plt.plot(F_val_skills,label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1-score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NTGRjVI2bhK"
   },
   "source": [
    "Load the best model on Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kKfbvWxfKi1O",
    "outputId": "9f081854-94a7-4bf7-833e-0f1d9e0d7d62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model reloaded !\n"
     ]
    }
   ],
   "source": [
    "#Reload a saved model, if parameter[\"reload\"] is set to a path\n",
    "load_fine_tuned = True\n",
    "path_fine_tuned_skills = './models/self-trained-model-skills'\n",
    "\n",
    "if load_fine_tuned:\n",
    "    model_skills.load_state_dict(torch.load(path_fine_tuned_skills, map_location=torch.device('cpu')))\n",
    "    print(\"Model reloaded !\")\n",
    "\n",
    "if use_gpu:\n",
    "    model_skills.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aR3oRcpI3bGI",
    "outputId": "88f86a32-5182-44aa-c497-5fb856c03a69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: new_F: 0.8357870894677237 best_F: 1 \n",
      "Validation: new_F: 0.75 best_F: 1 \n",
      "Test: new_F: 0.7845659163987139 best_F: 1 \n"
     ]
    }
   ],
   "source": [
    "_, new_train_F, _ =  evaluating(model_skills, train_data_skills, 1 , tag_to_id_skills, \"Training\")\n",
    "_, new_val_F, _ =  evaluating(model_skills, val_data_skills, 1 , tag_to_id_skills, \"Validation\")\n",
    "_, new_test_F, _ =  evaluating(model_skills, test_data_skills, 1 , tag_to_id_skills, \"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWcAsgKa2qP6"
   },
   "source": [
    "Prediction on the test dataset of Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-q4WRwrDU3x"
   },
   "outputs": [],
   "source": [
    "model_testing_sentences=[]\n",
    "for data in val_data_skills:\n",
    "    model_testing_sentences.append(' '.join(data['str_words']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hJTo2eNHKi1P",
    "outputId": "2e2fd069-245e-4775-b182-d2ec7d1f1e9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "LANGUES : O\n",
      "Anglais : ANGLAIS\n",
      "Courant : NIVEAU_ANGLAIS\n",
      "Espagnol : OTHER_LANGUAGE\n",
      "Debutant : O\n",
      "\n",
      "\n",
      "COMPETENCES : O\n",
      "Comptabilite : O\n",
      "et : O\n",
      "tenue : O\n",
      "des : O\n",
      "livres : O\n",
      "comptables : O\n",
      "clients : O\n",
      "et : O\n",
      "fournisseurs : O\n",
      "e : O\n",
      "Bilans : O\n",
      "comptables : O\n",
      "e : O\n",
      "Attenuation : O\n",
      "et : O\n",
      "gestion : O\n",
      "des : O\n",
      "risques : O\n",
      "e : O\n",
      "Gestion : O\n",
      "des : O\n",
      "donnees : O\n",
      "e : O\n",
      "Bon : O\n",
      "esprit : O\n",
      "d'analyse : O\n",
      "lmmOLS : O\n",
      "MCN : O\n",
      "VAg : O\n",
      "ual : O\n",
      "=tcX= : O\n",
      "Maitrise : O\n",
      "des : O\n",
      "logiciels : O\n",
      "informatiques : O\n",
      "de : O\n",
      "bureautique : O\n",
      "Excel : SKILLS\n",
      "Oracle : SKILLS\n",
      "Sge100 : SKILLS\n",
      "INFORMATIONS : SKILLS\n",
      "SUPPLEMENTAIRES : SKILLS\n",
      "e : O\n",
      "Langues : O\n",
      "Logiciels : O\n",
      "faatelhdaX-cm-an : O\n",
      "la] : O\n",
      "das : O\n",
      "d'Interets : O\n",
      "LANGUES : O\n",
      "Francais : O\n",
      "Courant : O\n",
      "Anglais : ANGLAIS\n",
      "se : O\n",
      "Intermediaire : O\n",
      "LOGICIELS : O\n",
      "Oracle : SKILLS\n",
      "Sage1000 : SKILLS\n",
      "Pack : SKILLS\n",
      "office : SKILLS\n",
      "e : O\n",
      "Tableaux : O\n",
      "croises : O\n",
      "dynamiques : O\n",
      "\n",
      "\n",
      "COMPETENCES : O\n",
      "ET : O\n",
      "OUTILS : O\n",
      "Gestion : O\n",
      "de : O\n",
      "comptes : O\n",
      "clients : O\n",
      "particuliers : O\n",
      "et : O\n",
      "professionnels : O\n",
      "Traitement : O\n",
      "des : O\n",
      "demandes : O\n",
      "de : O\n",
      "prets : O\n",
      "et : O\n",
      "credits : O\n",
      "bancaires : O\n",
      "Bonne : O\n",
      "maitrise : O\n",
      "des : O\n",
      "services : O\n",
      "financiers : O\n",
      "et : O\n",
      "bancaires : O\n",
      "Methodes : O\n",
      "de : O\n",
      "transactions : O\n",
      "financieres : O\n",
      "Connaissance : O\n",
      "des : O\n",
      "reglementations : O\n",
      "et : O\n",
      "des : O\n",
      "lois : O\n",
      "en : O\n",
      "vigueur : O\n",
      "Bonne : O\n",
      "maitrise : O\n",
      "de : O\n",
      "la : O\n",
      "fiscalite : O\n",
      "et : O\n",
      "la : O\n",
      "comptabilite : O\n",
      "Bonne : O\n",
      "maitrise : O\n",
      "de : O\n",
      "RH : O\n",
      "Bonne : O\n",
      "maitrise : O\n",
      "des : O\n",
      "methode : O\n",
      "de : O\n",
      "calcul : O\n",
      "des : O\n",
      "couts : O\n",
      "et : O\n",
      "stocks : O\n",
      "Microsoft : SKILLS\n",
      "Office : SKILLS\n",
      "PowerPoint : SKILLS\n",
      "Word : SKILLS\n",
      "Excel : SKILLS\n",
      "et : O\n",
      "Outlook : SKILLS\n",
      "Sage : SKILLS\n",
      "Ai-COMPTA : SKILLS\n",
      "LANGUES : O\n",
      "Arabe : OTHER_LANGUAGE\n",
      "Francais : O\n",
      "Anglais : ANGLAIS\n",
      "Allemands : NIVEAU_ANGLAIS\n",
      "\n",
      "\n",
      "Excel : SKILLS\n",
      "avance : O\n",
      "+ : O\n",
      "VBA : SKILLS\n",
      "Nfl : SKILLS\n",
      "ge : O\n",
      "PowerPoint : SKILLS\n",
      "WordPress : SKILLS\n",
      "Gestion : O\n",
      "administrative : O\n",
      "Gestion : O\n",
      "des : O\n",
      "RH : O\n",
      "Comptabilite : O\n",
      "Generale : O\n",
      "Facturation : O\n",
      "Notes : O\n",
      "de : O\n",
      "frais : O\n",
      "Office : O\n",
      "management : O\n",
      "Gestion : O\n",
      "base : O\n",
      "de : O\n",
      "donnees : O\n",
      "Accueil : O\n",
      "physique : O\n",
      "et : O\n",
      "telephonique : O\n",
      "Organisation : O\n",
      "evenementielle : O\n",
      "Gestion : O\n",
      "de : O\n",
      "materiel : O\n",
      "Organisation : O\n",
      "de : O\n",
      "deplacements : O\n",
      "\n",
      "\n",
      "Anglais : ANGLAIS\n",
      "courant : NIVEAU_ANGLAIS\n",
      "Allemand : OTHER_LANGUAGE\n",
      "notions : O\n",
      "\n",
      "\n",
      "COMPETENCES : O\n",
      "Pack : SKILLS\n",
      "Office : SKILLS\n",
      "Google : SKILLS\n",
      "APS : O\n",
      "LANGUES : O\n",
      "Frangaise : O\n",
      "Anglais : ANGLAIS\n",
      "Espagnol : OTHER_LANGUAGE\n",
      "\n",
      "\n",
      "Informatique : O\n",
      "Word : SKILLS\n",
      "/Excel/ : SKILLS\n",
      "Powerpoint : SKILLS\n",
      "Bonne : O\n",
      "maitrise : O\n",
      "Know : O\n",
      "Your : O\n",
      "Customer : O\n",
      "Bonne : O\n",
      "maitrise : O\n",
      "CRM : SKILLS\n",
      "Bonne : O\n",
      "maitrise : O\n",
      "\n",
      "\n",
      "Langues : O\n",
      "Francais : O\n",
      "Langue : O\n",
      "maternelle : O\n",
      "Anglais : ANGLAIS\n",
      "Bilingue : NIVEAU_ANGLAIS\n",
      "Allemand : OTHER_LANGUAGE\n",
      "Competence : O\n",
      "professionnelle : O\n",
      "Polonais : O\n",
      "Competence : O\n",
      "professionnelle : O\n",
      "\n",
      "\n",
      "LOGICIELS : O\n",
      "ORD : SKILLS\n",
      "EXCEL : SKILLS\n",
      "POWERPOINT : SKILLS\n",
      "SAGE : SKILLS\n",
      "1000 : O\n",
      "MAGGI : SKILLS\n",
      "MAARCH : SKILLS\n",
      "SAGE : SKILLS\n",
      "FRP : SKILLS\n",
      "ETAT : O\n",
      "COMPTABLE : O\n",
      "CEGID : SKILLS\n",
      "IBIZA : O\n",
      "MEG : O\n",
      "SILAE : O\n",
      "QUADRATUS : O\n",
      "RECEPT : O\n",
      "BANK : O\n",
      "EAMS : O\n",
      "ZOOM : O\n",
      "SAVOIR : O\n",
      "ETRE : O\n",
      "O : O\n",
      "O : O\n",
      "O : O\n",
      "O : O\n",
      "O : O\n",
      "ORGANISATION : O\n",
      "DYNAMIQUE : O\n",
      "AUTONOMIE : O\n",
      "Sl : O\n",
      "POC : O\n",
      "ae : O\n",
      "BON : O\n",
      "RELATIONEL : O\n",
      "LANGUES : O\n",
      "Y : O\n",
      "Francais : O\n",
      "Courant : O\n",
      "Me : O\n",
      "NAAT : O\n",
      "SEM : O\n",
      "\\Len : O\n",
      "ce1A : O\n",
      "\n",
      "\n",
      "PACK : SKILLS\n",
      "OFFICE : SKILLS\n",
      "WORD : SKILLS\n",
      "EXCEL,POWERPOINT : SKILLS\n",
      "ZOOM : SKILLS\n",
      "RESEAUX : SKILLS\n",
      "SOCIAUX : SKILLS\n",
      "Se : O\n",
      "ANGLAIS : ANGLAIS\n",
      "B1 : O\n",
      "ESPAGNOL : OTHER_LANGUAGE\n",
      "A2 : O\n",
      "LOGICIELS : O\n",
      "_- : O\n",
      "LANGUES : O\n",
      "-_- : O\n",
      "\n",
      "\n",
      "QUADRILINGUES : O\n",
      "CHINOIS : OTHER_LANGUAGE\n",
      ">> : O\n",
      "Mandarin : OTHER_LANGUAGE\n",
      "et : O\n",
      "dialectal : O\n",
      "non : O\n",
      "litteraire : O\n",
      "langue : O\n",
      "maternelle : O\n",
      "FRANCAIS : O\n",
      ">> : O\n",
      "Scolarisee : O\n",
      "en : O\n",
      "France : O\n",
      "depuis : O\n",
      "l'age : O\n",
      "de : O\n",
      "10 : O\n",
      "ans : O\n",
      ">> : O\n",
      "Traductrice : O\n",
      "chinois-frangais : O\n",
      "ESPAGNOL : OTHER_LANGUAGE\n",
      ">> : O\n",
      "Niveau : O\n",
      "intermediaire : O\n",
      "ANGLAIS : ANGLAIS\n",
      ">> : O\n",
      "Niveau : NIVEAU_ANGLAIS\n",
      "courant : O\n",
      "MAITRISE : O\n",
      "DES : O\n",
      "OUTILS : O\n",
      "Ih : O\n",
      "'FORMATIQUES : SKILLS\n",
      "Microsoft : SKILLS\n",
      "Office : SKILLS\n",
      ">> : O\n",
      "Word : SKILLS\n",
      "Excel : SKILLS\n",
      "recherche : O\n",
      "V : O\n",
      "tableau : O\n",
      "croisee : O\n",
      "dynamique) : O\n",
      "PowerPoint : SKILLS\n",
      "Outlook : SKILLS\n",
      "Access : SKILLS\n",
      "Logiciels : O\n",
      ">> : O\n",
      "Adobe : SKILLS\n",
      "Photoshop : SKILLS\n",
      "Ethnos : SKILLS\n",
      "NCSC : SKILLS\n",
      "a : O\n",
      "CLO : O\n",
      "MEOY : O\n",
      "1 : O\n",
      "OMESD : SKILLS\n",
      "IO : O\n",
      "eaCGr! : SKILLS\n",
      "Internet : O\n",
      ">> : O\n",
      "Google : SKILLS\n",
      "Drive : SKILLS\n",
      "Dropbox : SKILLS\n",
      "\n",
      "\n",
      "@ : O\n",
      "COMPETENCES : O\n",
      "Mise : O\n",
      "en : O\n",
      "place : O\n",
      "de : O\n",
      "maintenance : O\n",
      "preventive : O\n",
      "Gestion : O\n",
      "du : O\n",
      "service : O\n",
      "Active : O\n",
      "Directory : O\n",
      "Gestion : O\n",
      "du : O\n",
      "parc : O\n",
      "informatique : O\n",
      "Detecter : O\n",
      "et : O\n",
      "resoudre : O\n",
      "les : O\n",
      "anomalies : O\n",
      "des : O\n",
      "materielles : O\n",
      "informatiques : O\n",
      "TCP/IP : O\n",
      "Maitrise/Installation : O\n",
      "de : O\n",
      "Windows : O\n",
      "7 : O\n",
      "10,Server : O\n",
      "Gestion : O\n",
      "des : O\n",
      "mises : O\n",
      "a : O\n",
      "jour : O\n",
      "WSUS : O\n",
      "Gestion : O\n",
      "des : O\n",
      "licences : O\n",
      "Gestion : O\n",
      "d'Exchange : O\n",
      "Server : O\n",
      "et : SKILLS\n",
      "Office : SKILLS\n",
      "Mise : O\n",
      "en : O\n",
      "place : O\n",
      "Serveur : O\n",
      "\n",
      "\n",
      "CONNAISSANCE : O\n",
      "LINGUISTIQUE : O\n",
      "@ : O\n",
      "ANGLAIS : ANGLAIS\n",
      "NIVEAU : NIVEAU_ANGLAIS\n",
      "AVANCE : OTHER_LANGUAGE\n",
      "@ : O\n",
      "FRAncats : O\n",
      "\n",
      "\n",
      "LANGUES : O\n",
      "aris : O\n",
      "M@ : O\n",
      "Tamoul : O\n",
      "VW : O\n",
      "Espagnol : OTHER_LANGUAGE\n",
      "\n",
      "\n",
      "COMPETENCES : O\n",
      "SAISIE : O\n",
      "DES : O\n",
      "DONNEES : O\n",
      "Sle : O\n",
      "A : O\n",
      "GG : O\n",
      "a : O\n",
      "aE : O\n",
      "COMMUNICATION : O\n",
      "GESTION : O\n",
      "DE : O\n",
      "PLANNING : O\n",
      "CIEL : O\n",
      "COMPTABILITE/PAIE : O\n",
      "FACTURATION : O\n",
      "ACCUEIL : O\n",
      "TELEPHONIQUE : O\n",
      "RAITEMENT : O\n",
      "DES : O\n",
      "DOSSIERS : O\n",
      "BUREAUTIQUES : O\n",
      "\n",
      "\n",
      "3 : O\n",
      "EUR> : O\n",
      "COMPETENCES : O\n",
      "/ : O\n",
      "@ : O\n",
      "INFORMATIQUE : O\n",
      "SAGE : SKILLS\n",
      "ORACLE : SKILLS\n",
      "CIEL : SKILLS\n",
      "@@0ee@0 : O\n",
      "QUICK : SKILLS\n",
      "BOOKS : O\n",
      "PRO2008 : SKILLS\n",
      "@@@QO : O\n",
      "SAP : SKILLS\n",
      "@00@ee@0 : O\n",
      "POWERPOINT : SKILLS\n",
      "@eee@0 : O\n",
      "AFRI : SKILLS\n",
      "COMPTA : SKILLS\n",
      "@@ee@0O : O\n",
      "@ : O\n",
      "LANGUES : O\n",
      "| : O\n",
      "FRANGAIS : O\n",
      "COURANT : O\n",
      "00000 : O\n",
      "ANGLAIS : ANGLAIS\n",
      "MOYEN : NIVEAU_ANGLAIS\n",
      "@@@OoOO : NIVEAU_ANGLAIS\n",
      "@ : O\n",
      "PROFESSIONNELLES : O\n",
      "REALISATION : O\n",
      "D'UN : O\n",
      "SUIVI : O\n",
      "DE : O\n",
      "TRESORERIE : O\n",
      "COMPTABILITE : O\n",
      "ET : O\n",
      "CONTROLE : O\n",
      "DE : O\n",
      "GESTION : O\n",
      "GESTION : O\n",
      "DES : O\n",
      "STOCKS : O\n",
      "ET : O\n",
      "DES : O\n",
      "APPROVISIONNEMENTS : O\n",
      "CONNAISSANCE : O\n",
      "DE : O\n",
      "LA : O\n",
      "REGLEMENTATION : O\n",
      "SYSCOHADA : O\n",
      "\n",
      "\n",
      "NZ : O\n",
      "AN : O\n",
      "LANGUES : O\n",
      "Anglais : ANGLAIS\n",
      "Intermediare : NIVEAU_ANGLAIS\n",
      "Espagnol : OTHER_LANGUAGE\n",
      "Tres : O\n",
      "bon : O\n",
      "Logiciels : O\n",
      "maitrises : O\n",
      "; : O\n",
      "pack : O\n",
      "offices : O\n",
      "Google : SKILLS\n",
      "drive : SKILLS\n",
      "SAP : SKILLS\n",
      "CRM : SKILLS\n",
      "CUBE : SKILLS\n",
      "\n",
      "\n",
      "PREFERENCES : O\n",
      "PROFESSIONNELLES : O\n",
      "Logiciel : O\n",
      "maitrise : O\n",
      "Pack : SKILLS\n",
      "Microsoft : SKILLS\n",
      "Word/Excel/Powerpoint : SKILLS\n",
      "DO : O\n",
      "ood : O\n",
      "Logiciels : O\n",
      "de : O\n",
      "modeelisation : O\n",
      "Solidworks/Catia : SKILLS\n",
      "oo : O\n",
      "Autocad : O\n",
      "eee : O\n",
      "Climawin : SKILLS\n",
      "OOodoadda : SKILLS\n",
      "Retscreen : O\n",
      "eee : O\n",
      "\n",
      "\n",
      "COMPETENCES : O\n",
      "Savoir : O\n",
      "faire : O\n",
      "e : O\n",
      "Analyser : O\n",
      "les : O\n",
      "besoins : O\n",
      "du : O\n",
      "client : O\n",
      "e : O\n",
      "Definir : O\n",
      "la : O\n",
      "fiabilite : O\n",
      "et : O\n",
      "la : O\n",
      "rentabilite : O\n",
      "d'un : O\n",
      "projet : O\n",
      "e : O\n",
      "Valoriser : O\n",
      "des : O\n",
      "solutions : O\n",
      "techniques : O\n",
      "a : O\n",
      "partir : O\n",
      "de : O\n",
      "diagnostics : O\n",
      "energetiques : O\n",
      "sur : O\n",
      "des : O\n",
      "installations : O\n",
      "e : O\n",
      "Contreler : O\n",
      "la : O\n",
      "realisation : O\n",
      "d'un : O\n",
      "projet : O\n",
      "e : O\n",
      "Etudier : O\n",
      "un : O\n",
      "cahier : O\n",
      "des : O\n",
      "charges : O\n",
      "Connaissances : O\n",
      "e : O\n",
      "Construction : O\n",
      "durable : O\n",
      "e : O\n",
      "Energie : O\n",
      "renouvelable : O\n",
      "e : O\n",
      "Amenagement : O\n",
      "du : O\n",
      "batiment : O\n",
      "e : O\n",
      "Chiffrage : O\n",
      "projet : O\n",
      "<< : O\n",
      "Conception : O\n",
      "et : O\n",
      "Dessin : O\n",
      "assiste : O\n",
      "par : O\n",
      "ordinateur : O\n",
      "e : O\n",
      "Management : O\n",
      "de : O\n",
      "projet : O\n",
      "<< : O\n",
      "Developpement : O\n",
      "durable : O\n",
      "e : O\n",
      "Etude : O\n",
      "de : O\n",
      "marche : O\n",
      "<< : O\n",
      "Reglementation : O\n",
      "thermique : O\n",
      "e : O\n",
      "RT : O\n",
      "2012 : O\n",
      "C/ : O\n",
      "Certificat : O\n",
      "d'Economie : O\n",
      "d'Energie : O\n",
      "Niveaux : O\n",
      "Linguistiques : O\n",
      "e : O\n",
      "Anglais : ANGLAIS\n",
      "Niveau : NIVEAU_ANGLAIS\n",
      "Bl : NIVEAU_ANGLAIS\n",
      "e : O\n",
      "Espagnol : OTHER_LANGUAGE\n",
      "Niveau : O\n",
      "A2 : O\n",
      "\n",
      "\n",
      "LANGUES : O\n",
      "ET : O\n",
      "INFORMATIQUE : O\n",
      "Anglais : ANGLAIS\n",
      "Courant : NIVEAU_ANGLAIS\n",
      "Bonne : O\n",
      "maitrise : O\n",
      "du : O\n",
      "Pack : SKILLS\n",
      "office : SKILLS\n",
      "Word : SKILLS\n",
      "Excel : SKILLS\n",
      "Access : SKILLS\n",
      "Power : SKILLS\n",
      "point) : O\n",
      "\n",
      "\n",
      "Aa : O\n",
      "Aa : O\n",
      "Aa : O\n",
      "Aa : O\n",
      "BO : O\n",
      "a : O\n",
      "a : O\n",
      "Od : O\n",
      "saisie : O\n",
      "et : O\n",
      "reglement : O\n",
      "des : O\n",
      "factures : O\n",
      "verification : O\n",
      "en : O\n",
      "cas : O\n",
      "de : O\n",
      "relance : O\n",
      "emissions : O\n",
      "des : O\n",
      "virements : O\n",
      "recherche : O\n",
      "et : O\n",
      "analyse : O\n",
      "d'anomalie : O\n",
      "des : O\n",
      "flux : O\n",
      "contrdle : O\n",
      "des : O\n",
      "ecarts : O\n",
      "entre : O\n",
      "les : O\n",
      "comptes : O\n",
      "des : O\n",
      "titulaires : O\n",
      "et : O\n",
      "les : O\n",
      "donnees : O\n",
      "emises : O\n",
      "par : O\n",
      "les : O\n",
      "banques : O\n",
      "traitement : O\n",
      "mensuel : O\n",
      "des : O\n",
      "donnees : O\n",
      "de : O\n",
      "prestations : O\n",
      "sociales : O\n",
      "caf : O\n",
      "centre : O\n",
      "d'action : O\n",
      "sociale : O\n",
      "..) : O\n",
      "suivi : O\n",
      "de : O\n",
      "dossiers : O\n",
      "calcul : O\n",
      "en : O\n",
      "fonction : O\n",
      "des : O\n",
      "procedures : O\n",
      "mises : O\n",
      "en : O\n",
      "place : O\n",
      "remises : O\n",
      "de : O\n",
      "cheques : O\n",
      "numeerisation : O\n",
      "classement : O\n",
      "et : O\n",
      "archivage : O\n",
      "de : O\n",
      "documents : O\n",
      "\n",
      "\n",
      "Travaux : O\n",
      "courants : O\n",
      "de : O\n",
      "comptabilite : O\n",
      "Travaux : O\n",
      "d'inventaire : O\n",
      "et : O\n",
      "arrete : O\n",
      "des : O\n",
      "comptes : O\n",
      "Paies : O\n",
      "courantes : O\n",
      "et : O\n",
      "declarations : O\n",
      "sociales : O\n",
      "Declarations : O\n",
      "fiscales : O\n",
      "periodiques : O\n",
      "\n",
      "\n",
      "Word : O\n",
      "Excel : SKILLS\n",
      "Sage : SKILLS\n",
      "banque : O\n",
      "et : O\n",
      "tresorerie : O\n",
      "Studio : O\n",
      "Wincoge : SKILLS\n",
      "Coala : O\n",
      "\n",
      "\n",
      "COMPETENCES : O\n",
      "Anglais : ANGLAIS\n",
      "courant : NIVEAU_ANGLAIS\n",
      "lu : O\n",
      "ecrit : O\n",
      "parle) : O\n",
      "Accueil : O\n",
      "des : O\n",
      "clients : O\n",
      "et : O\n",
      "traitement : O\n",
      "des : O\n",
      "demandes : O\n",
      "telephoniques : O\n",
      "et : O\n",
      "e-mails : O\n",
      "Maitrise : O\n",
      "du : O\n",
      "Pack : SKILLS\n",
      "Microsoft : SKILLS\n",
      "Office : SKILLS\n",
      "grande : O\n",
      "aisance : O\n",
      "informatique : O\n",
      "Organisation : O\n",
      "des : O\n",
      "plannings : O\n",
      "et : O\n",
      "des : O\n",
      "deplacements : O\n",
      "des : O\n",
      "responsables : O\n",
      "dont : O\n",
      "la : O\n",
      "gestion : O\n",
      "des : O\n",
      "notes : O\n",
      "de : O\n",
      "frais) : O\n",
      "Suivi : O\n",
      "et : O\n",
      "rapprochement : O\n",
      "comptable : O\n",
      "avec : O\n",
      "l'Expert : O\n",
      "ou : O\n",
      "l'entreprise : O\n",
      "dediee : O\n",
      "Organisation : O\n",
      "des : O\n",
      "reunions : O\n",
      "d'equipe : O\n",
      "et : O\n",
      "redaction : O\n",
      "des : O\n",
      "comptes : O\n",
      "rendus : O\n",
      "et : O\n",
      "autres : O\n",
      "evenements : O\n",
      "seminaires) : O\n",
      "Gestion : O\n",
      "administrative : O\n",
      "de : O\n",
      "dossiers : O\n",
      "tous : O\n",
      "types : O\n",
      "suivi : O\n",
      "des : O\n",
      "delais : O\n",
      "de : O\n",
      "procedure : O\n",
      "gestion : O\n",
      "des : O\n",
      "priorites : O\n",
      "Redaction : O\n",
      "des : O\n",
      "contrats : O\n",
      "juridiques : O\n",
      "FR-EN : O\n",
      "licence : O\n",
      "d'exploitation : O\n",
      "statuts : O\n",
      "d'entreprise : O\n",
      "NDA : O\n",
      "business : O\n",
      "_ : O\n",
      "plan : O\n",
      "representation : O\n",
      "de : O\n",
      "mandataire : O\n",
      "cession : O\n",
      "d'inventeurs) : O\n",
      "QUALITES : O\n",
      "Rigueur : O\n",
      "Aisance : O\n",
      "relationnelle : O\n",
      "Discretion : O\n",
      "Autonomie : O\n",
      "\n",
      "\n",
      "COMPETENCES : O\n",
      "LINGUISTIQUES : O\n",
      "& : O\n",
      "TECHNIQUES : O\n",
      "e : O\n",
      "Langues : O\n",
      "Anglais : ANGLAIS\n",
      "Bon : NIVEAU_ANGLAIS\n",
      "niveau) : O\n",
      "Espagnol : OTHER_LANGUAGE\n",
      "niveau : O\n",
      "scolaire) : O\n",
      "Tamoul : O\n",
      "Langue : O\n",
      "Maternelle : O\n",
      "e : O\n",
      "Bureautique : O\n",
      "Pack : SKILLS\n",
      "Office : SKILLS\n",
      "e : O\n",
      "Langage : O\n",
      "de : O\n",
      "programmation : O\n",
      "HTML : SKILLS\n",
      "CSS : SKILLS\n",
      "PHP : SKILLS\n",
      "Langage : SKILLS\n",
      "C/C++ : SKILLS\n",
      "MySQL : SKILLS\n",
      "e : O\n",
      "Environnement : O\n",
      "de : O\n",
      "travail : O\n",
      "Windows : SKILLS\n",
      "10,8 : O\n",
      "7 : O\n",
      "Vista) : SKILLS\n",
      "Windows : SKILLS\n",
      "Server : O\n",
      "2012 : O\n",
      "R2 : O\n",
      "Linux : SKILLS\n",
      "iOS : SKILLS\n",
      "e : O\n",
      "Systemes : O\n",
      "& : O\n",
      "Reseaux : O\n",
      "Telephone : O\n",
      "IP : O\n",
      "Cisco : O\n",
      "Switch : SKILLS\n",
      "Routeur : SKILLS\n",
      "DHCP : SKILLS\n",
      "LAN : SKILLS\n",
      "VLAN : SKILLS\n",
      "VTP : SKILLS\n",
      "TCP/UDP : O\n",
      "Adressage : O\n",
      "IP : O\n",
      "Projet : O\n",
      "de : O\n",
      "BTS : O\n",
      "Y : O\n",
      "Stockage : O\n",
      "des : O\n",
      "stations : O\n",
      "de : O\n",
      "bus : O\n",
      "dans : O\n",
      "une : O\n",
      "base : O\n",
      "de : O\n",
      "donnees : O\n",
      "MySQL) : O\n",
      "Y : O\n",
      "Installation : O\n",
      "et : O\n",
      "codage : O\n",
      "du : O\n",
      "module : O\n",
      "logiciel : O\n",
      "qui : O\n",
      "permet : O\n",
      "de : O\n",
      "calculer : O\n",
      "les : O\n",
      "distances : O\n",
      "entre : O\n",
      "les : O\n",
      "arrets : O\n",
      "Y : O\n",
      "Utiliser : O\n",
      "la : O\n",
      "position : O\n",
      "du : O\n",
      "bus : O\n",
      "pour : O\n",
      "sortir : O\n",
      "la : O\n",
      "station : O\n",
      "la : O\n",
      "proche : O\n",
      "a : O\n",
      "partir : O\n",
      "des : O\n",
      "station : O\n",
      "stockees : O\n",
      "dans : O\n",
      "MySQL : SKILLS\n",
      "Y : O\n",
      "Envoie : O\n",
      "du : O\n",
      "nom : O\n",
      "de : O\n",
      "la : O\n",
      "station : O\n",
      "la : O\n",
      "proche : O\n",
      "Client/serveur) : O\n",
      "\n",
      "\n",
      "x : O\n",
      "Competences : O\n",
      "Analyse : O\n",
      "du : O\n",
      "besoin : O\n",
      "Definition : O\n",
      "du : O\n",
      "cahier : O\n",
      "des : O\n",
      "charges : O\n",
      "Sourcing : O\n",
      "fournisseurs : O\n",
      "Analyse : O\n",
      "des : O\n",
      "offres : O\n",
      "Negociation : O\n",
      "financiere/contractuelle : O\n",
      "Analyse : O\n",
      "de : O\n",
      "portefeuille : O\n",
      "Achats : O\n",
      "Analyse : O\n",
      "financiere : O\n",
      "\n",
      "\n",
      "A : O\n",
      "Langages : O\n",
      "Francais : O\n",
      "Anglais : ANGLAIS\n",
      "Espagnol : OTHER_LANGUAGE\n",
      "\n",
      "\n",
      "Cj : O\n",
      "Informatique : SKILLS\n",
      "OFFICE : SKILLS\n",
      "365 : O\n",
      "IVALUA : SKILLS\n",
      "SAP : SKILLS\n",
      "\n",
      "\n",
      "COMPETENCES : O\n",
      "Administration : O\n",
      "des : O\n",
      "ventes : O\n",
      "Comptabilite : O\n",
      "generale : O\n",
      "Contrele : O\n",
      "de : O\n",
      "gestion : O\n",
      "Informatique : O\n",
      "de : O\n",
      "gestion : O\n",
      "Logiciels : O\n",
      "de : O\n",
      "comptabilite : O\n",
      "Logiciels : O\n",
      "de : O\n",
      "gestion : O\n",
      "Management/formation : O\n",
      "Ressources : O\n",
      "humaines : O\n",
      "\n",
      "\n",
      "INFORMATIQUE : O\n",
      "Environnement : O\n",
      "Mac : O\n",
      "et : O\n",
      "PC : O\n",
      "Pack : SKILLS\n",
      "Office : SKILLS\n",
      "EBP : SKILLS\n",
      "Comptabilite : SKILLS\n",
      "EBP : SKILLS\n",
      "Gestion : O\n",
      "commerciale : O\n",
      "Silae : SKILLS\n",
      "SIFAC : SKILLS\n",
      "\n",
      "\n",
      "LANGUES : O\n",
      "PNET : O\n",
      "Espagnol : OTHER_LANGUAGE\n",
      "Indonesien : O\n",
      "\n",
      "\n",
      "Langues : O\n",
      "parlees : O\n",
      "Espagnol : OTHER_LANGUAGE\n",
      "Notions : O\n",
      "Anglais : ANGLAIS\n",
      "Scolaire : O\n",
      "\n",
      "\n",
      "Competences : O\n",
      "Verification : O\n",
      "constitution : O\n",
      "et : O\n",
      "traitement : O\n",
      "de : O\n",
      "dossiers : O\n",
      "Expert : O\n",
      "Utilisation : O\n",
      "d'outils : O\n",
      "bureautiques(traitement : O\n",
      "de : O\n",
      "textes,word) : O\n",
      "Niveau : O\n",
      "avance : O\n",
      "Assistance : O\n",
      "physique : O\n",
      "telephonique : O\n",
      "et : O\n",
      "prise : O\n",
      "de : O\n",
      "rendez-vous : O\n",
      "Niveau : O\n",
      "avance : O\n",
      "Gestion : O\n",
      "et : O\n",
      "traitement : O\n",
      "des : O\n",
      "reclamations : O\n",
      "et : O\n",
      "des : O\n",
      "litiges : O\n",
      "clients : O\n",
      "Niveau : O\n",
      "avance : O\n",
      "Gestion : O\n",
      "des : O\n",
      "dossiers : O\n",
      "du : O\n",
      "courrier : O\n",
      "archivage : O\n",
      "et : O\n",
      "classement) : O\n",
      "Expert : O\n",
      "Realisation : O\n",
      "de : O\n",
      "facturations,devis : O\n",
      "et : O\n",
      "gestions : O\n",
      "mailing : O\n",
      "Niveau : O\n",
      "avance : O\n",
      "\n",
      "\n",
      "KANNAN : O\n",
      "NNNAAN : O\n",
      "ee : O\n",
      "aaa : O\n",
      "ek : O\n",
      "Accueillir : O\n",
      "une : O\n",
      "clientele : O\n",
      "et : O\n",
      "filtrer : O\n",
      "des : O\n",
      "appels : O\n",
      "telephoniques : O\n",
      "Organiser : O\n",
      "le : O\n",
      "planning : O\n",
      "d'un : O\n",
      "responsable : O\n",
      "collaborateur : O\n",
      "Organiser : O\n",
      "des : O\n",
      "deplacements : O\n",
      "professionnels : O\n",
      "Preparer : O\n",
      "et : O\n",
      "organiser : O\n",
      "des : O\n",
      "reunions : O\n",
      "Realiser : O\n",
      "un : O\n",
      "suivi : O\n",
      "d'activite : O\n",
      "gerer : O\n",
      "des : O\n",
      "projets : O\n",
      "Prat : O\n",
      "1 : O\n",
      "mat : O\n",
      "eis : O\n",
      "mote : O\n",
      "melee : O\n",
      "alae : O\n",
      "Me : O\n",
      "ey : O\n",
      "ele : O\n",
      "SCC : O\n",
      "mealies : O\n",
      "Manteca : O\n",
      "mee : O\n",
      "em : O\n",
      "eee : O\n",
      "yale : O\n",
      "=e : O\n",
      "Coordonner : O\n",
      "l'activite : O\n",
      "d'une : O\n",
      "equipe : O\n",
      "\n",
      "\n",
      "\\? : O\n",
      "sa : O\n",
      "O? : O\n",
      "Qualites : O\n",
      "Aisance : O\n",
      "relationnelle : O\n",
      "Organisee : O\n",
      "Discretion : O\n",
      "Rigoureuse : O\n",
      "Bienveillante : O\n",
      "Sens : O\n",
      "de : O\n",
      "l'organisation : O\n",
      "et : O\n",
      "de : O\n",
      "|'ecoute : O\n",
      "Travail : O\n",
      "en : O\n",
      "equipe : O\n",
      "Yah : O\n",
      "Yael : O\n",
      "Courant : O\n",
      "Espagnol : OTHER_LANGUAGE\n",
      "Courant : O\n",
      "Bureautique : O\n",
      "Pack : SKILLS\n",
      "Office : SKILLS\n",
      "Concur : SKILLS\n",
      "Salesforce : SKILLS\n",
      "Oracle : SKILLS\n",
      "\n",
      "\n",
      "A : O\n",
      "Langues : O\n",
      "Anglais : ANGLAIS\n",
      "Niveau : NIVEAU_ANGLAIS\n",
      "courant : NIVEAU_ANGLAIS\n",
      "C1 : NIVEAU_ANGLAIS\n",
      "Espagnol : OTHER_LANGUAGE\n",
      "Niveau : O\n",
      "intermediaire : O\n",
      "B1 : O\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#parameters\n",
    "lower=parameters['lower']\n",
    "\n",
    "#preprocessing\n",
    "final_test_data = []\n",
    "for sentence in model_testing_sentences:\n",
    "    s=sentence.split()\n",
    "    str_words = [w for w in s]\n",
    "    \n",
    "    words = [word_to_id[lower_case(w,lower) if lower_case(w,lower) in word_to_id else '<UNK>'] for w in str_words]    \n",
    "    \n",
    "    \n",
    "    # Skip characters that are not in the training set\n",
    "    chars = [[char_to_id[c] for c in w if c in char_to_id] for w in str_words]\n",
    "    \n",
    "    final_test_data.append({\n",
    "        'str_words': str_words,\n",
    "        'words_id': words,\n",
    "        'chars': chars,\n",
    "    })\n",
    "\n",
    "#prediction\n",
    "predictions = []\n",
    "print(\"Prediction:\")\n",
    "for data in final_test_data:\n",
    "    words = data['str_words']\n",
    "    chars2 = data['chars']\n",
    "\n",
    "    d = {} \n",
    "    \n",
    "    # Padding the each word to max word size of that sentence\n",
    "    chars2_length = [len(c) for c in chars2]\n",
    "    char_maxl = max(chars2_length)\n",
    "    chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
    "    for i, c in enumerate(chars2):\n",
    "        chars2_mask[i, :chars2_length[i]] = c\n",
    "    chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "\n",
    "    dwords = Variable(torch.LongTensor(data['words_id']))\n",
    "\n",
    "    # We are getting the predicted output from our model\n",
    "    if use_gpu:\n",
    "        val,predicted_id = model_skills(dwords.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
    "    else:\n",
    "        val,predicted_id = model_skills(dwords, chars2_mask, chars2_length, d)\n",
    "    pred_chunks = get_chunks(predicted_id,tag_to_id_skills)\n",
    "    temp_list_tags=['O']*len(words)\n",
    "    for p in pred_chunks:\n",
    "        nb_compo = p[2] - p[1]\n",
    "        for c in range(nb_compo):\n",
    "            temp_list_tags[p[1]+c]=p[0]\n",
    "      \n",
    "        \n",
    "    for word,tag in zip(words,temp_list_tags):\n",
    "        print(word,':',tag)\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NER_CV_light.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
